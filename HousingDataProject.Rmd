---
title: "Housing Data Project"
author: "Justin Sheldon, Jeremy Swiatek"
date: "October 24, 2018"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: united
    highlight: tango
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(boot)
library(car)
library(plyr)
library(dummies)
```
#Abstract
The puprose of this project is to predict the "Sale Price" of a house using some or all of the 79 explanatory variables in our data set. We used (name the different models used once we are finished) to find the model with the best predictive power.(talk a little about our best model). In addition to predicting "Sale Price" we also isolated (list of significant variables) that had the highest impact of Sale Price. (Talk about each and how it effect Sale Price). In conclusion we now know a little bit more about what effects the sale price of a house and can make more informed decisions on the biggest purchase most people make in their lifetime.

#Introduction
Location, Location, Location, is that all you really need to know about housing to make an informed decision on the value of a house? Being an informed consumer is always a good idea but especially on the largest purchase you will probably make in your lifetime. The goal of this project is to be able to predict the sale price of a house with a reasonably good accuracy. Just as important, if not more so, we also want to gain a deeper understanding of the relationship between different factors and the sale price of a house.

Another goal of this project is to learn more about and get experience with the different statistical learning methods we have covered in class this semester. We plan to try out a lot of the methods we covered in class this semester and discuss the strengths and weaknesses of each method as it pertains to our data set. Therefore this project will be broken up into sections for each of the different methods. There is a table of contents to the right that you can click to jump to any section you want to look at. We will try to keep each section as self contained as possible.


#The Data
The dataset we will be using is the House Prices dataset from www.kaggle.com. The Response Variable is SalePrice and there are 79 explanatory variables. There are 1460 observations in total in the training Data. We did a lot of data cleaning before building any models. Below is everything we did catagorized into sections. After all the data cleaning we ended up with a training set with ......... This is the training set we will be using for all of our models.

```{r echo = FALSE}
trainData = read.csv("train.csv", stringsAsFactors = FALSE)
testData = read.csv("test.csv", stringsAsFactors = FALSE)
```

###  Dealing with NA's
Upon further inspection of the data we noticed we had some variables that were causing us trouble. There are 19 variables with missing data. For a lot of the categorical variables "NA's" really meant "None" so we fixed those by converting NA level to a "None" level. There was also a couple of observations that had one missing value for a given variable and we just fixed that by predicting the mean/mode of that variable for that observation. Any other additional modification is listed below along with the reason why we choose that modification.

* **LotFrontage:** Linear Feet of street connected to property. This one was a tough decision. It's missing 259/1460 approximately 18% of the data for this variable but we didnt want to lose the variable entirely. We decided to fill in the NA's with the mean value of the variable.
* **GarageYrBlt:** Garage Year Built. We replaced the NA's with the Year the house was built as long as it actualy had a garage. We expect GarageYrBlt and YearBuilt to be highly correlated and eventually one will probably have to be removed from the model anyways.
* **MiscFeature:** Miscellaneous feature. This variable was essentially worthless. Almost all of the data was missing and the data that did exist was 'tennis court' and 'other'. We removed this variable.

```{r, Finding the NAs, include=  FALSE, comment = ""}
NAVariables <- which(colSums(is.na(trainData)) > 0)
sort(colSums(sapply(trainData[NAVariables], is.na)), decreasing = TRUE)
```

```{r, Fixing NA Variables, echo=FALSE, include=FALSE}
#Fixing Pool QC
trainData$PoolQC[is.na(trainData$PoolQC)] = 'None'
Qualities = c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)
trainData$PoolQC = as.integer(revalue(trainData$PoolQC, Qualities))

#Fixing Alley
trainData$Alley[is.na(trainData$Alley)] = 'None'
trainData$Alley = as.factor(trainData$Alley)

#Fixing Fence
trainData$Fence[is.na(trainData$Fence)] = 'None'
trainData$Fence = as.factor(trainData$Fence)

#Fixing Fireplace
trainData$FireplaceQu[is.na(trainData$FireplaceQu)] = 'None'
trainData$FireplaceQu = as.integer(revalue(trainData$FireplaceQu, Qualities))

#Fixing LotFrontage and other lot variables
for (i in 1:nrow(trainData)){
        if(is.na(trainData$LotFrontage[i])){
               trainData$LotFrontage[i] = as.integer(mean(trainData$LotFrontage, na.rm = TRUE))
        }
}
trainData$LotShape = as.integer(revalue(trainData$LotShape, c('IR3'=0, 'IR2'=1, 'IR1'=2, 'Reg'=3)))
trainData$LotConfig = as.factor(trainData$LotConfig)

#Fixing Garage variables
#Year Built
trainData$GarageYrBlt[is.na(trainData$GarageYrBlt)] = trainData$YearBuilt[is.na(trainData$GarageYrBlt)]
#Garage Type
trainData$GarageType[is.na(trainData$GarageType)] = 'None'
trainData$GarageType = as.factor(trainData$GarageType)
#Garage Finish
trainData$GarageFinish[is.na(trainData$GarageFinish)] = 'None'
FinishLevels = c('None'=0, 'Unf'=1, 'RFn'=2, 'Fin'=3)
trainData$GarageFinish<-as.integer(revalue(trainData$GarageFinish, FinishLevels))
#Garage Quality
trainData$GarageQual[is.na(trainData$GarageQual)] = 'None'
trainData$GarageQual = as.integer(revalue(trainData$GarageQual, Qualities))
#Garage Condition
trainData$GarageCond[is.na(trainData$GarageCond)] = 'None'
trainData$GarageCond = as.integer(revalue(trainData$GarageCond, Qualities))

#Fixing Basement Variables
#Additional NA in these two observations
trainData[!is.na(trainData$BsmtFinType1) & (is.na(trainData$BsmtCond)|is.na(trainData$BsmtQual)|is.na(trainData$BsmtExposure)|is.na(trainData$BsmtFinType2)), c('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')]
#Predict Most common occurence to fix these two observations
trainData$BsmtFinType2[333] = "Unf"
trainData$BsmtExposure[949] = "No"
#Basement Quality
trainData$BsmtQual[is.na(trainData$BsmtQual)] = 'None'
trainData$BsmtQual = as.integer(revalue(trainData$BsmtQual, Qualities))
#Basement Condition
trainData$BsmtCond[is.na(trainData$BsmtCond)] = 'None'
trainData$BsmtCond = as.integer(revalue(trainData$BsmtCond, Qualities))
#Basement Exposure
trainData$BsmtExposure[is.na(trainData$BsmtExposure)] = 'None'
trainData$BsmtExposure = as.factor(trainData$BsmtExposure)
#Basement Finish Type 1
trainData$BsmtFinType1[is.na(trainData$BsmtFinType1)] = 'None'
trainData$BsmtFinType1 = as.factor(trainData$BsmtFinType1)
#Basement Finish Type 2
trainData$BsmtFinType2[is.na(trainData$BsmtFinType2)] = 'None'
trainData$BsmtFinType2 = as.factor(trainData$BsmtFinType2)

#Fixing Masonry Variables
#Masonary Veneer Type
trainData$MasVnrType[is.na(trainData$MasVnrType)] = 'None'
trainData$MasVnrType = as.factor(trainData$MasVnrType)
#Masonary Veneer Area
trainData$MasVnrArea[is.na(trainData$MasVnrArea)] = 0

#Fixing Electrical
trainData$Electrical[is.na(trainData$Electrical)] = "SBrkr"
trainData$Electrical = as.factor(trainData$Electrical)


#Fixing MiscFeature (junk just remove)
trainData = subset(trainData, select = -MiscFeature)
```

```{r, Fixing Character Variables, echo=FALSE,include=FALSE}
#Turning Char variables into factor variables
trainData$MSZoning = as.factor(trainData$MSZoning)
trainData$Street = as.factor(trainData$Street)
trainData$LandContour = as.factor(trainData$LandContour)
trainData$Neighborhood = as.factor(trainData$Neighborhood)
trainData$Condition1 = as.factor(trainData$Condition1)
trainData$BldgType = as.factor(trainData$BldgType)
trainData$HouseStyle = as.factor(trainData$HouseStyle)
trainData$RoofStyle = as.factor(trainData$RoofStyle)
trainData$Exterior1st = as.factor(trainData$Exterior1st)
trainData$Exterior2nd = as.factor(trainData$Exterior2nd)
trainData$Foundation = as.factor(trainData$Foundation)
trainData$Heating = as.factor(trainData$Heating)
trainData$SaleType = as.factor(trainData$SaleType)
trainData$SaleCondition = as.factor(trainData$SaleCondition)


#Turning Char Variables into Ordinal Numeric
trainData$LandSlope = as.integer(revalue(trainData$LandSlope, c('Sev'=0, 'Mod'=1, 'Gtl'=2)))
trainData$ExterQual = as.integer(revalue(trainData$ExterQual, Qualities))
trainData$ExterCond = as.integer(revalue(trainData$ExterCond, Qualities))
trainData$HeatingQC = as.integer(revalue(trainData$HeatingQC, Qualities))
trainData$CentralAir = as.integer(revalue(trainData$CentralAir, c('N'=0, 'Y'=1)))
trainData$KitchenQual = as.integer(revalue(trainData$KitchenQual, Qualities))
trainData$Functional = as.integer(revalue(trainData$Functional, c('Sal'=0, 'Sev'=1, 'Maj2'=2, 'Maj1'=3, 'Mod'=4, 'Min2'=5, 'Min1'=6, 'Typ'=7)))
trainData$PavedDrive = as.integer(revalue(trainData$PavedDrive, c('N'=0, 'P'=1, 'Y'=2)))

#Not a part of the data just observation #
trainData = subset(trainData, select = -Id)

#Make sure Empty
charVariables = names(trainData[,sapply(trainData, is.character)])
charVariables
```

###  Multicollinearity
We decided to just deal with multicollinearity issues before building any models. We checked the correlation matrix for all numeric variables and decided to remove any pair of variables with a correlation of 0.75 or higher. Of the two variables we kept Whichever variable had a higher correlation with SalePrice. Below is a list of all of these pairs and which variables we kept.

* **Garage Yr Blt vs. Year Built:** correlation = 0.845. We kept Year Built
* **Garage Area vs. Garage Cars:** correlation = 0.882. We kept Garage Cars
* **Total Basement SqFt vs. 1st Floor SqFt** correlation = 0.819. We kept Total Basement SqFt
* **Garage Condition vs. Garage Quality** correlation = 0.959. We kept Garage Quality
* **Total Rooms Above Ground vs. Ground Living Area** correlation = 0.825. We kept Ground Living Area

```{r Multicollinearity, include=FALSE, echo=FALSE}
#Finding the Correlated Variables
    # numericVars <- which(sapply(trainData, is.numeric))
    # all_numVar <- trainData[, numericVars]
    # #Garage Year Build vs YearBuilt
    # print(cor(all_numVar, trainData$GarageYrBlt) > 0.75)
    # print(cor(trainData$GarageYrBlt, trainData$SalePrice))
    # print(cor(trainData$YearBuilt, trainData$SalePrice))
    # #Garage Garage Cars vs GarageArea
    # print(cor(trainData$GarageArea, trainData$GarageCars))
    # print(cor(trainData$GarageArea, trainData$SalePrice))
    # print(cor(trainData$GarageCars, trainData$SalePrice))
    # #TotalBsmtSF vs. X1stFlrSF
    # print(cor(trainData$TotalBsmtSF, all_numVar) > 0.75)
    # print(cor(trainData$TotalBsmtSF, trainData$X1stFlrSF))
    # print(cor(trainData$TotalBsmtSF, trainData$SalePrice))
    # print(cor(trainData$X1stFlrSF, trainData$SalePrice))
    # #GarageCond vs. Garage Quality
    # print(cor(trainData$GarageCond, all_numVar) > 0.75)
    # print(cor(trainData$GarageCond, trainData$GarageQual))
    # print(cor(trainData$GarageCond, trainData$SalePrice))
    # print(cor(trainData$GarageQual, trainData$SalePrice))
    # #Total Rooms Above Ground vs. Ground Living Area
    # print(cor(trainData$TotRmsAbvGrd, all_numVar) > 0.75)
    # print(cor(trainData$TotRmsAbvGrd, trainData$GrLivArea))
    # print(cor(trainData$TotRmsAbvGrd, trainData$SalePrice))
    # print(cor(trainData$GrLivArea, trainData$SalePrice))
    
#Remove the Variables
trainData = subset(trainData, select = -GarageYrBlt)
trainData = subset(trainData, select = -GarageArea)
trainData = subset(trainData, select = -X1stFlrSF)
trainData = subset(trainData, select = -GarageCond)
trainData = subset(trainData, select = -TotRmsAbvGrd)

#Check Removed
#dim(trainData)
```

###  Dealing with Sparse Data
Some of our factor variables had levels with a count of less then 5. This was causing problems with using cross validation. Below is a list of all the variables that had this issue and how we dealth with them.

* **Utilities:** Type of utilities available. 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Condition2:** Proximity to various conditions. 8 levels with 99% of the data belonging to one level. We removed this variable.
* **RoofMatl1:** Roof Material. 8 levels with more than 98% of the data belonging to one level. We removed this variable.
* **Street:** 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Heating:** 6 levels with 98% of the data belonging to one level. We removed this variable.

We kept the rest of the variables that had sparse levels in the model because they had other levels that were not sparse and could be important in future models. We converted all the factor variables into dummy variables and then just removed the dummy variables with levels that had less than 10 observations. This ended up being 32 out of 162 levels.

```{r Looking at Catagorical Variables, echo=FALSE, include=FALSE}
#Find Categorical Variables
numericVars = which(sapply(trainData, is.numeric))
all_factorVars = trainData[,-numericVars]
print(names(all_factorVars))
```
```{r Sparse Data, include=FALSE,echo=FALSE}
#Too Sparse < 2% of data not in same category
trainData = subset(trainData, select = -Utilities)
trainData = subset(trainData, select = -Condition2)
trainData = subset(trainData, select = -RoofMatl)
trainData = subset(trainData, select = -Street)
trainData = subset(trainData, select = -Heating)
```
```{r Dummy Variables, include=FALSE, echo=FALSE}
#Find all the factor variables
factorVars = which(sapply(trainData, is.factor))
factorVars = names(factorVars)
#Create seperate data frame to convert factor variables to dummy varaibles
DFfactors = trainData[, factorVars]
DFdummies = dummy.data.frame(DFfactors, sep = ".")
#Find Sparse dummy variables
sparseLevels <- which(colSums(DFdummies[1:nrow(trainData[!is.na(trainData$SalePrice),]),])<10)
#Remove Sparse dummy variables
DFdummies = DFdummies[,-sparseLevels]
#combine all the non factor variables in original data frame with our new dummy variables
DFnumeric = trainData[,!(names(trainData) %in% factorVars)]
trainDF.dummies = cbind(DFnumeric, DFdummies)
```

###  Removing Outliers

###  Fixing Skewed Response Variable


#Multiple Linear Regression
The main purpose of building a Multiple Linear Regression model is to use it as a benchmark that we compare all future models to. Below is a brief summary of our Multiple Linear Regression model that includes all variables in the model fit. 

* 10 Fold Cross Valiation Error Rate: (waiting for clean data)
* Most Significant Parameters: 
* Non-Significant Parameters: 

```{r, Base Model, echo=FALSE, include=FALSE}
baseModel = glm(SalePrice ~ ., data = trainDF.dummies)
summary(baseModel)
```

```{r, Cross Validation, comment="", include=FALSE, echo=FALSE}
set.seed(1)
cvTrainData = trainDF.dummies
cv.error10 = cv.glm(cvTrainData, baseModel, K=10)$delta[1]
```

##  Potential Problems

####     Residual Plots
From the Residual plots it appears as if a linear model may not be a good representation of the data. We plan to try out some Model selection and regularization methods and see if this pattern still exists. If so we will use non-linear methods.

```{r, Model Residuals, echo=FALSE, include=FALSE}
par(mfrow= c(2,2))
plot(baseModel)
```

####     Model Outliers
After looking at the residual plot we noticed there are 10 outliers that may be influencing the model fit. The points influncing the data are listed below. We will keep an eye on these observations in future models and see if they continue to be a problem. We will deal with them later if we need to.
```{r, Model Outliers, echo=FALSE, comment=""}
outlierTest(baseModel)
```



#Model Selection and Regularization

##Subset Selection
####Forward Stepwise Selection
```{r, Forward Stepwise Selection, include=FALSE, echo=FALSE}

```

####Backwards Stepwise Selection
```{r, Backwards Stepwise Selection, include=FALSE, echo=FALSE}

```



##Shrinkage Methods
####Ridge Regression
```{r, Ridge Regression, include=FALSE, echo=FALSE}

```

####Lasso Regression
```{r, Lasso, include=FALSE, echo=FALSE}

```



##Dimension Reduction Methods
####Principle Components Regression
```{r, PCR, include=FALSE, echo=FALSE}

```

####Partial Least Squares
```{r, PLS, include=FALSE, echo=FALSE}

```