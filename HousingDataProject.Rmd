title: "Housing Data Project"
author: "Justin Sheldon, Jeremy Swiatek"
date: "October 24, 2018"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: united
    highlight: tango
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(boot)
library(car)
library(plyr)
library(dummies)
library(ggplot2)
library(gridExtra)
library(leaps)
library(ggrepel)
library(ggthemes)
library(glmnet)
library(pls)
library(psych)
library(gam)
summary(baselibrary(splines)
```
#Introduction
Location, Location, Location, is that all you really need to know about housing to make an informed decision on the value of a house? Being an informed consumer is always a good idea but especially on the largest purchase you will probably make in your lifetime. The goal of this project is to be able to predict the sale price of a house with a reasonably good accuracy. Just as important, if not more so, we also want to gain a deeper understanding of the relationship between different factors and the sale price of a house.

The puprose of this project is to predict the "Sale Price" of a house using  79 predictor variables in our data set ranging from Lot area and shape to neighborhood and year biult. The response variable we are trying to predict is Sales Price using some of the 79 predictor variables that we find to be significant in the data. We will use techniques from the Introduction to Statistcial Learning book to perform different trypes of regression on the data to make a prediction on Sales Price.

#The Goal
We want to clean the data so that we have a proper model that gives good predictions of Sales price. We want to use (name the different models used once we are finished) to find the model with the best predictive power. In addition to predicting Sale Price using generalized linear regression, we want to isolated significant variables to see which had the highest impact on Sale Price. We also want to see which variables aren't significant and find an explanation for why it doesn't affect the data.

Another goal of this project is to learn more about and get experience with the different statistical learning methods we have covered in class this semester. We plan to try out a lot of the methods we covered in class this semester and discuss the strengths and weaknesses of each method as it pertains to our data set. Therefore this project will be broken up into sections for each of the different methods. There is a table of contents to the right that you can click to jump to any section you want to look at. We will try to keep each section as self contained as possible.


#The Data
The dataset we will be using is the House Prices dataset from www.kaggle.com. The Response Variable is SalePrice and there are 79 explanatory variables. There are 1460 observations in total in the training Data. We did a lot of data cleaning before building any models. Below is everything we did catagorized into sections. After all the data cleaning we ended up with a training set with 1458 observations and 178 variables (after dummy variable creation). This is the training set we will be using for all of our models.

```{r echo = FALSE}
trainData = read.csv("train.csv", stringsAsFactors = FALSE)
testData = read.csv("test.csv", stringsAsFactors = FALSE)
```

###  Dealing with NA's
Upon further inspection of the data we noticed we had some variables that were causing us trouble. There are 19 variables with missing data. For a lot of the categorical variables "NA's" really meant "None" so we fixed those by converting NA level to a "None" level. There was also a couple of observations that had one missing value for a given variable and we just fixed that by predicting the mean/mode of that variable for that observation. Any other additional modification is listed below along with the reason why we choose that modification.

* **LotFrontage:** Linear Feet of street connected to property. This one was a tough decision. It's missing 259/1460 approximately 18% of the data for this variable but we didnt want to lose the variable entirely. We decided to fill in the NA's with the mean value of the variable.
* **GarageYrBlt:** Garage Year Built. We replaced the NA's with the Year the house was built as long as it actualy had a garage. We expect GarageYrBlt and YearBuilt to be highly correlated and eventually one will probably have to be removed from the model anyways.
* **MiscFeature:** Miscellaneous feature. This variable was essentially worthless. Almost all of the data was missing and the data that did exist was 'tennis court' and 'other'. We removed this variable.

```{r, Finding the NAs, include=  FALSE, comment = ""}
NAVariables <- which(colSums(is.na(trainData)) > 0)
sort(colSums(sapply(trainData[NAVariables], is.na)), decreasing = TRUE)
```
```{r, Fixing NA Variables, echo=FALSE, include=FALSE}
#Fixing Pool QC
trainData$PoolQC[is.na(trainData$PoolQC)] = 'None'
Qualities = c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)
trainData$PoolQC = as.integer(revalue(trainData$PoolQC, Qualities))

#Fixing Alley
trainData$Alley[is.na(trainData$Alley)] = 'None'
trainData$Alley = as.factor(trainData$Alley)

#Fixing Fence
trainData$Fence[is.na(trainData$Fence)] = 'None'
trainData$Fence = as.factor(trainData$Fence)

#Fixing Fireplace
trainData$FireplaceQu[is.na(trainData$FireplaceQu)] = 'None'
trainData$FireplaceQu = as.integer(revalue(trainData$FireplaceQu, Qualities))

#Fixing LotFrontage and other lot variables
for (i in 1:nrow(trainData)){
        if(is.na(trainData$LotFrontage[i])){
               trainData$LotFrontage[i] = as.integer(mean(trainData$LotFrontage, na.rm = TRUE))
        }
}
trainData$LotShape = as.integer(revalue(trainData$LotShape, c('IR3'=0, 'IR2'=1, 'IR1'=2, 'Reg'=3)))
trainData$LotConfig = as.factor(trainData$LotConfig)

#Fixing Garage variables
#Year Built
trainData$GarageYrBlt[is.na(trainData$GarageYrBlt)] = trainData$YearBuilt[is.na(trainData$GarageYrBlt)]
#Garage Type
trainData$GarageType[is.na(trainData$GarageType)] = 'None'
trainData$GarageType = as.factor(trainData$GarageType)
#Garage Finish
trainData$GarageFinish[is.na(trainData$GarageFinish)] = 'None'
FinishLevels = c('None'=0, 'Unf'=1, 'RFn'=2, 'Fin'=3)
trainData$GarageFinish<-as.integer(revalue(trainData$GarageFinish, FinishLevels))
#Garage Quality
trainData$GarageQual[is.na(trainData$GarageQual)] = 'None'
trainData$GarageQual = as.integer(revalue(trainData$GarageQual, Qualities))
#Garage Condition
trainData$GarageCond[is.na(trainData$GarageCond)] = 'None'
trainData$GarageCond = as.integer(revalue(trainData$GarageCond, Qualities))

#Fixing Basement Variables
#Additional NA in these two observations
trainData[!is.na(trainData$BsmtFinType1) & (is.na(trainData$BsmtCond)|is.na(trainData$BsmtQual)|is.na(trainData$BsmtExposure)|is.na(trainData$BsmtFinType2)), c('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')]
#Predict Most common occurence to fix these two observations
trainData$BsmtFinType2[333] = "Unf"
trainData$BsmtExposure[949] = "No"
#Basement Quality
trainData$BsmtQual[is.na(trainData$BsmtQual)] = 'None'
trainData$BsmtQual = as.integer(revalue(trainData$BsmtQual, Qualities))
#Basement Condition
trainData$BsmtCond[is.na(trainData$BsmtCond)] = 'None'
trainData$BsmtCond = as.integer(revalue(trainData$BsmtCond, Qualities))
#Basement Exposure
trainData$BsmtExposure[is.na(trainData$BsmtExposure)] = 'None'
trainData$BsmtExposure = as.factor(trainData$BsmtExposure)
#Basement Finish Type 1
trainData$BsmtFinType1[is.na(trainData$BsmtFinType1)] = 'None'
trainData$BsmtFinType1 = as.factor(trainData$BsmtFinType1)
#Basement Finish Type 2
trainData$BsmtFinType2[is.na(trainData$BsmtFinType2)] = 'None'
trainData$BsmtFinType2 = as.factor(trainData$BsmtFinType2)

#Fixing Masonry Variables
#Masonary Veneer Type
trainData$MasVnrType[is.na(trainData$MasVnrType)] = 'None'
trainData$MasVnrType = as.factor(trainData$MasVnrType)
#Masonary Veneer Area
trainData$MasVnrArea[is.na(trainData$MasVnrArea)] = 0

#Fixing Electrical
trainData$Electrical[is.na(trainData$Electrical)] = "SBrkr"
trainData$Electrical = as.factor(trainData$Electrical)


#Fixing MiscFeature (junk just remove)
trainData = subset(trainData, select = -MiscFeature)
```
```{r, Fixing Character Variables, echo=FALSE,include=FALSE}
#Turning Char variables into factor variables
trainData$MSZoning = as.factor(trainData$MSZoning)
trainData$Street = as.factor(trainData$Street)
trainData$LandContour = as.factor(trainData$LandContour)
trainData$Neighborhood = as.factor(trainData$Neighborhood)
trainData$Condition1 = as.factor(trainData$Condition1)
trainData$BldgType = as.factor(trainData$BldgType)
trainData$HouseStyle = as.factor(trainData$HouseStyle)
trainData$RoofStyle = as.factor(trainData$RoofStyle)
trainData$Exterior1st = as.factor(trainData$Exterior1st)
trainData$Exterior2nd = as.factor(trainData$Exterior2nd)
trainData$Foundation = as.factor(trainData$Foundation)
trainData$Heating = as.factor(trainData$Heating)
trainData$SaleType = as.factor(trainData$SaleType)
trainData$SaleCondition = as.factor(trainData$SaleCondition)


#Turning Char Variables into Ordinal Numeric
trainData$LandSlope = as.integer(revalue(trainData$LandSlope, c('Sev'=0, 'Mod'=1, 'Gtl'=2)))
trainData$ExterQual = as.integer(revalue(trainData$ExterQual, Qualities))
trainData$ExterCond = as.integer(revalue(trainData$ExterCond, Qualities))
trainData$HeatingQC = as.integer(revalue(trainData$HeatingQC, Qualities))
trainData$CentralAir = as.integer(revalue(trainData$CentralAir, c('N'=0, 'Y'=1)))
trainData$KitchenQual = as.integer(revalue(trainData$KitchenQual, Qualities))
trainData$Functional = as.integer(revalue(trainData$Functional, c('Sal'=0, 'Sev'=1, 'Maj2'=2, 'Maj1'=3, 'Mod'=4, 'Min2'=5, 'Min1'=6, 'Typ'=7)))
trainData$PavedDrive = as.integer(revalue(trainData$PavedDrive, c('N'=0, 'P'=1, 'Y'=2)))

#Not a part of the data just observation #
trainData = subset(trainData, select = -Id)

#Make sure Empty
charVariables = names(trainData[,sapply(trainData, is.character)])
charVariables
```

###  Dealing with Multicollinearity
We decided to just deal with multicollinearity issues before building any models. We checked the correlation matrix for all numeric variables and decided to remove any pair of variables with a correlation of 0.75 or higher. Of the two variables we kept Whichever variable had a higher correlation with SalePrice. Below is a list of all of these pairs and which variables we kept.

* **Garage Yr Blt vs. Year Built:** correlation = 0.845. We kept Year Built
* **Garage Area vs. Garage Cars:** correlation = 0.882. We kept Garage Cars
* **Total Basement SqFt vs. 1st Floor SqFt** correlation = 0.819. We kept Total Basement SqFt
* **Garage Condition vs. Garage Quality** correlation = 0.959. We kept Garage Quality
* **Total Rooms Above Ground vs. Ground Living Area** correlation = 0.825. We kept Ground Living Area

```{r Multicollinearity, include=FALSE, echo=FALSE}
#Finding the Correlated Variables
    # numericVars <- which(sapply(trainData, is.numeric))
    # all_numVar <- trainData[, numericVars]
    # #Garage Year Build vs YearBuilt
    # print(cor(all_numVar, trainData$GarageYrBlt) > 0.75)
    # print(cor(trainData$GarageYrBlt, trainData$SalePrice))
    # print(cor(trainData$YearBuilt, trainData$SalePrice))
    # #Garage Garage Cars vs GarageArea
    # print(cor(trainData$GarageArea, trainData$GarageCars))
    # print(cor(trainData$GarageArea, trainData$SalePrice))
    # print(cor(trainData$GarageCars, trainData$SalePrice))
    # #TotalBsmtSF vs. X1stFlrSF
    # print(cor(trainData$TotalBsmtSF, all_numVar) > 0.75)
    # print(cor(trainData$TotalBsmtSF, trainData$X1stFlrSF))
    # print(cor(trainData$TotalBsmtSF, trainData$SalePrice))
    # print(cor(trainData$X1stFlrSF, trainData$SalePrice))
    # #GarageCond vs. Garage Quality
    # print(cor(trainData$GarageCond, all_numVar) > 0.75)
    # print(cor(trainData$GarageCond, trainData$GarageQual))
    # print(cor(trainData$GarageCond, trainData$SalePrice))
    # print(cor(trainData$GarageQual, trainData$SalePrice))
    # #Total Rooms Above Ground vs. Ground Living Area
    # print(cor(trainData$TotRmsAbvGrd, all_numVar) > 0.75)
    # print(cor(trainData$TotRmsAbvGrd, trainData$GrLivArea))
    # print(cor(trainData$TotRmsAbvGrd, trainData$SalePrice))
    # print(cor(trainData$GrLivArea, trainData$SalePrice))
    
#Remove the Variables
trainData = subset(trainData, select = -GarageYrBlt)
trainData = subset(trainData, select = -GarageArea)
trainData = subset(trainData, select = -X1stFlrSF)
trainData = subset(trainData, select = -GarageCond)
trainData = subset(trainData, select = -TotRmsAbvGrd)

#Check Removed
#dim(trainData)
```

###  Dealing with Sparse Data
Some of our factor variables had levels with a count of less then 5. This was causing problems with using cross validation. Below is a list of all the variables that had this issue and how we dealth with them.

* **Utilities:** Type of utilities available. 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Condition2:** Proximity to various conditions. 8 levels with 99% of the data belonging to one level. We removed this variable.
* **RoofMatl1:** Roof Material. 8 levels with more than 98% of the data belonging to one level. We removed this variable.
* **Street:** 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Heating:** 6 levels with 98% of the data belonging to one level. We removed this variable.

We kept the rest of the variables that had sparse levels in the model because they had other levels that were not sparse and could be important in future models. We converted all the factor variables into dummy variables and then just removed the dummy variables with levels that had less than 10 observations. This ended up being 32 out of 162 levels.

```{r Looking at Catagorical Variables, echo=FALSE, include=FALSE}
#Find Categorical Variables
numericVars = which(sapply(trainData, is.numeric))
all_factorVars = trainData[,-numericVars]
print(names(all_factorVars))
```
```{r Remove Worthless Variables, include=FALSE,echo=FALSE}
#Too Sparse < 2% of data not in same category
trainData = subset(trainData, select = -Utilities)
trainData = subset(trainData, select = -Condition2)
trainData = subset(trainData, select = -RoofMatl)
trainData = subset(trainData, select = -Street)
trainData = subset(trainData, select = -Heating)
```
```{r Dummy Variables and new Train DF, include=FALSE, echo=FALSE}
#Find all the factor variables
factorVars = which(sapply(trainData, is.factor))
factorVars = names(factorVars)
#Create seperate data frame to convert factor variables to dummy varaibles
DFfactors = trainData[, factorVars]
DFdummies = dummy.data.frame(DFfactors, sep = ".")
#Find Sparse dummy variables
sparseLevels <- which(colSums(DFdummies[1:nrow(trainData[!is.na(trainData$SalePrice),]),])<10)
#Remove Sparse dummy variables
DFdummies = DFdummies[,-sparseLevels]
#combine all the non factor variables in original data frame with our new dummy variables
DFnumeric = trainData[,!(names(trainData) %in% factorVars)]
trainDF.dummies = cbind(DFnumeric, DFdummies)
```



###  Dealing with Skewed Response Variable
The response variable SalePrice was heavily right skewed. We fixed this with a log transformation. Below is a couple before and after graphs showing the results. We will be using the log(SalePrice) as our predictor variable in our models.

```{r Create Log Sale Price Data Frame, include=FALSE, echo=FALSE}
trainDF.log = trainDF.dummies
trainDF.log$SalePrice = log(trainDF.log$SalePrice)
```
```{r Plot of SalePrice vs. Normal Distribution , echo=FALSE , fig.align = "center", fig.height = 5, fig.width = 7}
p1 = ggplot(trainDF.dummies, aes(x=SalePrice)) +
        geom_histogram(aes(y = ..density..), fill="blue", binwidth = 10000) +
        stat_function(fun = dnorm, args = list(mean(trainDF.dummies$SalePrice),sd(trainDF.dummies$SalePrice)), colour = "red", lwd = 1) +
        scale_x_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000")) +
        labs(title = "SalePrice vs. Normal Distribution")

p2 = ggplot(trainDF.log, aes(x=SalePrice)) +
        geom_histogram(aes(y = ..density..), fill="blue", binwidth = .15) +
        stat_function(fun = dnorm, args = list(mean(trainDF.log$SalePrice),sd(trainDF.log$SalePrice)), colour = "red", lwd = 1) +
        scale_x_continuous(breaks= seq(10, 14, by=.5)) +
        labs(title = "Log SalePrice vs. Normal Distribution")

grid.arrange(p1, p2, nrow = 1, ncol = 2)
```
```{r QQ Plots, echo=FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
#QQ PLots
par(mfrow = c(1,2))
qqnorm(trainDF.dummies$SalePrice, main = "Sale Price QQ Plot")
qqline(trainDF.dummies$SalePrice)
qqnorm(trainDF.log$SalePrice, main = "Log Sale Price QQ Plot")
qqline(trainDF.log$SalePrice)
```

### Regularization of predictors
We converted all the numeric predictors that has a skewness rating of greater then absolute value of 1 by taking the log of them. This had the effect of removing any skewed tails and effectively normailzing the predictors.


### Dealing with Outliers
The only outliers we found were observations 524 and 1299. We deemed these outliers significant enough to remove because they were huge houses with a overall quality rating of 10 (the highest rating) and yet they sold for a very low price. By looking at the size and quality rating of the house if appears they may have missed a 0 on the end of the price. Size of the house (GrLiveArea) and Overall Quality are the two highest positive correlated variables with SalePrice which lead us to believe these SalePrices were a mistake and we removed these observations from the data set. We left the graphs below in terms of the actual Sale Price and not log of the Sale Price so it would be easier to interpret

```{r Finding Outliers, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6, comment=""}
ggplot(data=trainDF.dummies[!is.na(trainDF.dummies$SalePrice),], aes(x=GrLivArea, y=SalePrice))+
        geom_point(col='blue') + geom_smooth(method = "lm", se=FALSE, color="red", aes(group=1)) +
        scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000") ) +
        geom_text_repel(aes(label = ifelse(trainDF.dummies$GrLivArea[!is.na(trainDF.dummies$SalePrice)]>4500, rownames(trainDF.dummies), '')))
trainDF.dummies[c(524, 1299), c('SalePrice', 'GrLivArea', 'OverallQual')]
```

```{r final Data to be used for models, echo=FALSE, include=FALSE}
#Use cleanData for all future models
cleanData = trainDF.log
cleanData = cleanData[-c(524,1299),]
```


#Multiple Linear Regression
The main purpose of building a Multiple Linear Regression model is to use it as a benchmark that we compare all future models to. Below is a brief summary of our Multiple Linear Regression model that includes all variables in the model fit. By using the aov we can look at the main effects of each variable on the model to see which variables are most important and we did the model without aov to get a prediction model. 

* 10 Fold Cross Valiation RMSE: $21,767.80

```{r, Base Model, echo=FALSE, include=FALSE}
#Just use this model for Potential Problems Section
set.seed(88)
sample <- sample.int(n = nrow(cleanData), size = floor(.75*nrow(cleanData)), replace = F)
train.mlr <- cleanData[sample, ]
test.mlr  <- cleanData[-sample, ]
baseModel = glm(SalePrice ~ ., data = train.mlr)
mlr.pred = predict(baseModel, newdata = test.mlr)
mean((exp(mlr.pred) - exp(test.mlr$SalePrice))^2)^(1/2)
summary(aov(baseModel))
summary(baseModel)
```
##Model Assumptions
Here we are assuming that independance, Normality, and constant variance are true. Independance is the only assumption that is "forced" to be true becasue we are saying that each indiviudual house is not affected by the homes around it. We know that in general this is not true becasue the area people live in tends to affect how their home looks, however we can assume indepedance for the reason that every home is individual, seperate, and different that the ones around it. Normailiy and constant varience are much easier to say that their okay here becasue the distribution of homes and home prices is constant and their are very few outliers in the data set. Therefore all assumptions are met for the purpose of the model.

##Not Significant variables
Variables that weren't significant in Model: MSSubClass, ExterCond, BsmtCond, BsmtHalfBath, FullBath, FireplaceQu, WoodDeckSF, OpenPorchSF, EnclosedPorch, X3SsnPorch, PoolArea, PoolQC, MiscVal, MoSold, YrSold, MSZoning.RH, Alley.Grvl, Alley.None, LandContour.Bnk, LandContour.Low, LotConfig.Corner, LotConfig.FR2, LotConfig.Inside, Neighborhood.Blmngtn, Neighborhood.BrDale, Neighborhood.BrkSide, Neighborhood.CollgCr, Neighborhood.Edwards, Neighborhood.Gilbert, Neighborhood.IDOTRR, Neighborhood.Mitchel, Neighborhood.NAmes, Neighborhood.NoRidge, Neighborhood.OldTown, Neighborhood.Sawyer, Neighborhood.Somerst, Neighborhood.SWISU, Neighborhood.Timber, Condition1.PosN, Condition1.RRAn, Condition1.Feedr, BldgType.1Fam, BldgType.Duplex, HouseStyle.1.5Fin, HouseStyle.1.5Unf, HouseStyle.1Story, HouseStyle.2.5Unf, HouseStyle.2Story, HouseStyle.SFoyer, RoofStyle.Flat, RoofStyle.Gable, RoofStyle.Gambrel, RoofStyle.Hip, Exterior1st.AsbShng, Exterior1st.CemntBd, Exterior1st.HdBoard, Exterior1st.Plywood, Exterior1st.Stucco, Exterior1st.VinylSd, Exterior1st.WdShing, Exterior1st.Wd Sdng, Exterior2nd.AsbShng, Exterior2nd.CmentBd, Exterior2nd.HdBoard, Exterior2nd.ImStucc, Exterior2nd.MetalSd, Exterior2nd.Plywood, Exterior2nd.Stucco, Exterior2nd.VinylSd, Exterior2nd.Wd Shng, MasVnrType.BrkFace, Foundation.CBlock, Foundation.PConc, Foundation.Slab, BsmtExposure.Av, BsmtExposure.Mn, BsmtExposure.No, BsmtFinType1.ALQ, BsmtFinType1.BLQ, BsmtFinType1.GLQ, BsmtFinType1.LwQ, BsmtFinType1.Rec, BsmtFinType2.ALQ, BsmtFinType2.GLQ, BsmtFinType2.LwQ, BsmtFinType2.Rec, Electrical.FuseA, Electrical.FuseF, Electrical.SBrkr, GarageType.Attchd, GarageType.Basment, GarageType.BuiltIn, GarageType.Detchd, GarageType.None, Fence.GdPrv, Fence.MnPrv, Fence.MnWw, SaleType.COD, SaleType.WD, SaleCondition.Alloca, SaleCondition.Partial, SaleCondition.Normal

Based on the variables that weren't sigificant in the model there is a trend between variables related to housing exterior, Neighborhood, basement type, and garage type. This most likely occurs becasue in this area many of the homes are very similar meaning they could have very similar exteriors and each neighborhood could have housese that look very similar. This is what we would expect in smaller communities, a smaller variance in the types of homes in the area (more homes are about the same with a few very wealthy ones and few in worse "poorer" quality). Based on this we could expect that variables that are very similar in nature are discribing about the ame things so we don't need every version of the variable to make a prediction of sales price.

##Significant Variables
All othe models not include above were more significant and add significant infromation to the model. In order for a variable to be significant in the model the p-value has to be less than our type 1 error of .05. These variables are most important beacsue they provide the model with infromation that will make predicting more accurate beacsue they're the most informative variables. Vaiables like OverallCond and OverallQual are considered significant becasue they provide information on the quality/condition of the home which most people would consider to be important factors.

##Prediction Model
Using the summary of our model we got the intercepts of each variables so that we can use the data to make a prediction on sales price. In order to make the best model we excluded all the variables above that weren't significant in the data becasue we don't want redundancy in the model and we don't want to overfit the data when predicting.

###The Model
9.282 + (.00044)LotFrontage + (.0000019)LotArea + (.04577)OverallQual + (.03728)OverallCond + (.001533)YearBiult + (.0007532)YearRemodAdd +...+ (-.1391)SaleCondition.Abnorml + (-.06048)SaleCondition.Family

The 9.282 is our Beta0, it's the intercept of the model that tells us where to start for all homes in the area, and theoretically if all the variables were equal to 0 our home price would be 9.282. The numbers in the parenthesis are the individual Beta's associated with interpects for each variable. They each predict the individual affect of each variable and for every one unit increase in the variable it goes up by that intercept. In this case we have many variables that could have very high numbers which could be why we get such low beta's or it could be due to having many variables in the data that all the significant variables are putting in a little bit of infromation.

###Model Accuracy
By taking out all the non-significant variables in the data we know that our model is more accurate than with them in it. Since we got a very low AIC of -1688 we know the model is at the desired level of prediction on the fewest variables possible so we know we have the best model. So, due to the low AIC and high sample size of data we know that this is the best and most accurate model that we could have produced to predict Sale Price.

##  Potential Problems

####     Residual Plots
From the Residual plots it appears as if a linear model may not be a good representation of the data. We plan to try out some Model selection and regularization methods and see if this pattern still exists. If so we will use non-linear methods.

```{r, Model Residuals, echo=FALSE, include=FALSE}
par(mfrow= c(2,2))
plot(baseModel)
```

####     Model Outliers
After looking at the residual plot we noticed there are 10 outliers that may be influencing the model fit. The points influncing the data are listed below. We will keep an eye on these observations in future models and see if they continue to be a problem. We will deal with them later if we need to.
```{r, Model Outliers, echo=FALSE, comment=""}
outlierTest(baseModel)
```



#Model Selection and Regularization

##Subset Selection
The goal of subset Selection is to try and find the smallest subset of the predictors that are statistically significant and still lead to a good model fit. The advantage to an approach like this over Multiple Linear Regression is that using a subset of the original predictors can lead to a significant reduction in variance while only slightly increasing the bias. Also it simplifies model inference and kind of allows us to see which variables are "most important". Subset Selection still uses Least Squares fits in the form of  $Y = \beta_0 +\sum_{i=1}^p \beta_i X_i + \epsilon$.  Our hope is that a subset selection model will out perform the full model we used in Multiple Linear Regression Section. Below we perform both Forward stepwise Selection and Backwards Stepwise Selection and give more details about the methods used and the results.

```{r Stuff We Need for Subset Selection, include=FALSE, echo=FALSE}
subSetData = cleanData
```

####Forward Stepwise Selection
To perform forward stepwise selection we fit a model that iteratively steps forward adding the next most significant predictor at each iteration. We then used Cross Validation on each model from 1-177 predictors to find the best model size for our data. We also looked at the CP, BIC and Adjusted R^2 scores to decide on the best model size. Below is graphs of all 4 methods we used in picking the best model size.

```{r, Forward Stepwise Selection, include=FALSE, echo=FALSE}
#CP, BIC, AdjR^2
regfit.fwd = regsubsets(SalePrice ~., data = subSetData, nvmax = 160, method = "forward")
regfit.fwd.summary = summary(regfit.fwd)

#validation set 
set.seed(1)
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train <- subSetData[sample, ]
test  <- subSetData[-sample, ]

regfit.valid = regsubsets(SalePrice ~., data = train, nvmax = 160, method = "forward")
test.matrix = model.matrix(SalePrice ~., data = test)
validation.errors = rep(NA, 160)
for (i in 1:160){
    coefi = coef(regfit.valid, id=i)
    pred = test.matrix[,names(coefi)]%*%coefi
    validation.errors[i] = mean((test$SalePrice-pred)^2)
}

#Get Test RMSE on Sale Price
coefi2 = coef(regfit.valid, id=50)
pred2 = test.matrix[,names(coefi2)]%*%coefi2
mean(( exp(test$SalePrice) - exp(pred2) )^2)^(1/2)
```

```{r plots to include, echo=FALSE}
#set xlab abd ylab and points
par(mfrow = c(2,2))
plot(regfit.fwd.summary$adjr2, type = "l", xlab = "Number of Variables", ylab = "Adjusted R Squared")
points(which.max(regfit.fwd.summary$adjr2), regfit.fwd.summary$adjr2[which.max(regfit.fwd.summary$adjr2)], col="red", cex=2,pch=20)
plot(regfit.fwd.summary$bic, type = "l", ylab = "BIC", xlab = "Number of Variables")
points(which.min(regfit.fwd.summary$bic), regfit.fwd.summary$bic[which.min(regfit.fwd.summary$bic)], col="red", cex=2,pch=20)
plot(regfit.fwd.summary$cp, type = "l", ylab = "CP", xlab = "Number of Variables")
points(which.min(regfit.fwd.summary$cp), regfit.fwd.summary$cp[which.min(regfit.fwd.summary$cp)], col="red", cex=2,pch=20)
plot(validation.errors, type = "l", ylab = "Validation MSE", xlab = "Number of Variables")
points(which.min(validation.errors), validation.errors[which.min(validation.errors)], col="red",cex=2,pch=20)
```

Forward stepwise Selection revealed that the best model using this method is somewhere around 40-50 variables in the model. Remember this is not necessarly the best model because forward stepwise can get stuck in a suboptimal path. That being said it appears like around 50 variables will turn out to be a good model. Below is a report of the model with 50 variables vs the full Least Squares Model.

* **Best Model Size:** 50
* **Test RMSE FWD Stepwise Selction:** $22,584.54
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67
*We changed the Sale Price back into dollars instead of log(SalePrice) because it was easier to interpret. Also note this is the Root Mean Squared Error which can be thought of as how close our predictions are on average.

Surprisingly this model did very poorly. As we kept increasing the number of predictors the validation set RMSE kept decreasing. It appears our dataset does not have a problem with high variance and the reduction in variance we gained from a smaller model did not outweigh the increase in bias we incurred from using a smaller model. 

The most interesting thing we found from doing forward stepwise was just determining which variables were selected first and therefore are potentially important variables in predicting sale price.

**First 10 Variables picked in model**

* **OverallQual:** Rates the overall material and finish of the house.
* **BsmtFullBath:** Number of full bathrooms in the basement.
* **YearBuilt:** Original Construction Date.
* **OverallCond:** Rates the overall condition of the house.
* **GarageQual:** Rates the Quality of the Garage.
* **HeatingQC:** Heating Quality and Condition Rating.
* **Alley.none:** Dummy Variable representing no alleyway access to the house.
* **BsmtHalfBath:** Number of half bathrooms in the basement.
* **GarageFinish:** Rating of Interior finish of the garage.
* **MSZoning.FV:** Dummy Variable for Zoning Code for "Floating Village Residential"


####Backwards Stepwise Selection
The goal, methods and equation for backwards stepwise selection is the same as that of forward stepwise selection so we will not restate them all here.

```{r, Backwards Stepwise Selection, include=FALSE, echo=FALSE}
#CP, BIC, AdjR^2
regfit.back = regsubsets(SalePrice ~., data = subSetData, nvmax = 160, method = "backward")
regfit.back.summary = summary(regfit.back)

#validation set 
set.seed(1)
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train <- subSetData[sample, ]
test  <- subSetData[-sample, ]

regfit.valid = regsubsets(SalePrice ~., data = train, nvmax = 160, method = "backward")
test.matrix = model.matrix(SalePrice ~., data = test)
validation.errors = rep(NA, 160)
for (i in 1:160){
    coefi = coef(regfit.valid, id=i)
    pred = test.matrix[,names(coefi)]%*%coefi
    validation.errors[i] = mean((test$SalePrice-pred)^2)
}
coefi2 = coef(regfit.valid, id=50)
pred2 = test.matrix[,names(coefi2)]%*%coefi2
mean(( exp(test$SalePrice) - exp(pred2) )^2)^(1/2)

```
```{r plots for backstep, echo=FALSE}
par(mfrow = c(2,2))
plot(regfit.back.summary$adjr2, type = "l", xlab = "Number of Variables", ylab = "Adjusted R Squared")
points(which.max(regfit.back.summary$adjr2), regfit.back.summary$adjr2[which.max(regfit.back.summary$adjr2)], col="red", cex=2,pch=20)
plot(regfit.back.summary$bic, type = "l", ylab = "BIC", xlab = "Number of Variables")
points(which.min(regfit.back.summary$bic), regfit.back.summary$bic[which.min(regfit.back.summary$bic)], col="red", cex=2,pch=20)
plot(regfit.back.summary$cp, type = "l", ylab = "CP", xlab = "Number of Variables")
points(which.min(regfit.back.summary$cp), regfit.back.summary$cp[which.min(regfit.back.summary$cp)], col="red", cex=2,pch=20)
plot(validation.errors, type = "l", ylab = "Validation MSE", xlab = "Number of Variables")
points(which.min(validation.errors), validation.errors[which.min(validation.errors)], col="red",cex=2,pch=20)
```

Backwards Stepwise Selection ends up with pretty much the same results as forward stepwise selection. Somewhere between 40-50 variables seems to be when the graphs begin to flatten out. We decided to build a model with 50 predictors and use a validation set to estimate the RMSE. Below are the results.

* **Best Model Size:** 50
* **Test RMSE FWD Stepwise Selction:** $23341.39
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67

This model outperforms Forward Stepwise Selection but still does not outperform the Full Least Squares model. Again it appears like the reduction in varaiance we gain from fitting a simpler model do not outweigh the increase in bias.

**Last 10 Variables left in model**

* **OverallQual:** Rates the overall material and finish of the house.
* **BsmtFullBath:** Number of full bathrooms in the basement.
* **YearBuilt:** Original Construction Date.
* **BsmtFinSF1:** How many square feet of the basement that is finished.
* **OverallCond:** Rates the overall condition of the house.
* **GarageQual:** Rates the Quality of the Garage.
* **BsmtUnfSF:** Unfinished square feet of basement area.
* **LotArea:** Lot Size in Square Feet
* **BsmtFinSF2:** Rating of basment finished area.
* **Neighborhood.MeadowV:** Neighboorhood location Meadow Village

I think it is interesting to note that both subset selection methods find that having a finished basement with bathrooms seems to be important factor in predicting SalePrice (after accounting for Overall Quality). Also we see that a neighborhood appeared in the top 10 here. The coefficient for this neighborhood is negative so we dug a little deeper to see this neighboorhoods relationship with sale price in comparison to other neighborhoods. As you can see from the plot below Meadown Village has the lowest median sale price of all neighborhoods therefore this coefficient makes sense. Also it makes sense that Meadow Village would be a significant factor in predicting sale price because it appears to be one of the worst neighborhoods in terms of sales price in Ames Iowa.

```{r a look at Neighborhoods, echo=FALSE}
neighbor.plot = ggplot(trainData, aes(Neighborhood, SalePrice)) +
                geom_boxplot(aes(fill = factor(Neighborhood)))  +
                theme(axis.text.x = element_text(angle = 65, vjust = 0.6)) +
                labs( x = "Neighborhoods", y = "Sale Price") +
                theme(legend.position="none") +
                scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000") ) +
                geom_hline(yintercept = 88000, linetype="dashed", color = "red")
neighbor.plot
```

##Shrinkage Methods
####Ridge Regression
The goal of Ridge Regression is to find a set of coefficients that minimize  $RSS +\lambda\sum_{j=1}^p \beta^2_j$.  The summation terms works as a shrinkage penalty that incentivises the model to srink the coefficients towards 0. The hope is that by doing this we will significantly reduce model variance while only slightly increasing model bias and therefore be able to outperform the full Least Squares model. 

The equation aboves shrinkage penalty depends on the value of lambda you choose. In order to find the best value for lambda we performed cross validation on a lot of different lambda values. Below is a graph of the coefficients for different levels of lambda along with the best lambda we found using Cross Validation. (note: the function we use to fit the models automatically performs variable regularization)

```{r, Ridge Regression, include=FALSE, echo=FALSE}
#Setup
set.seed(1)
x = model.matrix(SalePrice ~., cleanData)[,-1]
y = cleanData$SalePrice
train = sample(1:nrow(x), nrow(x)/1.5)
test = (-train)
y.test= y[test]
ridge.mod = glmnet(x[train,],y[train],alpha = 0)

#Find best lamda value
cv.out = cv.glmnet(x[train,],y[train], alpha = 0)
bestLambda = cv.out$lambda.min
bestLambda

#test RMSE for best lambda
ridge.pred = predict(ridge.mod, s = bestLambda, newx = x[test,])
(mean((exp(ridge.pred)-exp(y.test))^2))^(1/2)

#baseline null model predictions
mean(( mean(exp(y[train])) -exp(y.test) )^2)^(1/2)
```
```{r, Ridge Regression plot, echo=FALSE}
plot(ridge.mod, xvar="lambda")
legend(1.75, -.125, legend=c("Best Lambda", "MSZoning.C (all)", "Neighborhood.MeadowV", "Exterior1st.BrkFace", "Neighborhood.Crawfor", "Neighborhood.StoneBr"),
       col=c("red","magenta", "black", "red", "green", 69), lty=c(2,1,1,1,1,1), cex=1)
abline(v = log(bestLambda), col = "red", lwd = 2, lty = 2)
```

The best lambda we found is close to 0 and therefore it looks like this model will be close to the full Least Squares model. This is not a huge surprise because we noticed in our subset selection models that a lot of predictors were significant. Also our sample size is a lot larger then the number of predictors 1458 >> 177 so reducing variance may not be a huge concern for us and the increase in bias may be a bigger problem. Below is a comparison in the model accuracies.

* **Best Lambda:** 0.044
* **Test RMSE Ridge:** $21,646.23
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67

The Ridge Regression model and the least squares model perform pretty much identically on a validation set at lamda = 0.044. They both significantly beat the Null Model that only uses the mean of Sale Price to predict Sale Price but essentially they are pretty much the same model.

####Lasso Regression
The goal of Lasso Regression is similar to Ridge Regression but in Lasso Regression we also perform subset selection. This has the benefit of potentially reducing variance and therefore improving prediction accuracy while also improving interpretibilty. We hope this model will outperform the full Least Squares while also being a lot more interpretible than Ridge Regression. The equation we use to do this is similar to Ridge Regression but now we minimize $RSS +\lambda\sum_{j=1}^p\lvert \beta_j \rvert$. Again we need to find the best lambda and we use the same methods as before.


```{r, Lasso, include=FALSE, echo=FALSE}
#Use train and test from before 

#Baseline Lasso Model
lasso.mod = glmnet(x[train,], y[train], alpha = 1)

#Find best Lambda
cv.lasso.out = cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.lasso.out)
bestLassoLambda = cv.lasso.out$lambda.min
bestLassoLambda

#Lasso Predictions accuracy
lasso.pred = predict(lasso.mod, s = bestLassoLambda, newx = x[test,])
(mean((exp(lasso.pred)-exp(y.test))^2))^(1/2)

#Number of Non Zero coefficients at best lambda
lasso.coef = predict(lasso.mod, type = "coefficients", s=bestLassoLambda)
sum(lasso.coef!=0)
```

```{r, Lasso Regression plot, echo=FALSE}
plot(lasso.mod, xvar="lambda")
legend(-4, -.275, legend=c("Best Lambda", "MSZoning.C (all)", "Neighborhood.MeadowV", "Neighborhood.Crawfor"), col=c("red", 69, "black", "blue" ), lty=c(2,1,1,1), pt.cex=1,  cex = 0.75)
abline(v = log(bestLassoLambda), col = "red", lwd = 2, lty = 2)
```

The Lasso model with the lowest cross validation error rate is a model with 71 predictors. This is close to we saw in forward and backward stepwise selection. 

* **Best Lambda:** 0.0039
* **Test RMSE Lasso:** $20,248.17
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67
* **Count of Non-Zero Coefficients:** 71

Although this helps with inference and interpretibility because of having fewer variables it doesn't help all that much with prediction. Again the Test RMSE is approximately the same between the Lasso model and the full Least Squares model. Currently this is the best model for both prediction and interpretibility we have.

##Dimension Reduction Methods
####Principle Components Regression
The goals of principle components regression is to find principle components of the form  Z = $\sum_{i=1}^n \phi_jX_j$  such that a few principle components can be used to predict the response. The hope is that this will lead to a substantial reduction in variance while only causing a slight increase in bias. Note that each principle component is a linear cobination of all the predictors. We want this method to outperform the Least Squares. This has an advantage over Least Squares in that it will have a lower variance but it will have a higher bias because we are assuming a simpler model. 

We fit a Principle Components Regression model (with scale = TRUE to regularize the data) and used cross validation to determine the optimal number of principal components. Below is a plot of this model fit. After finding the optimal number of components we fit a new model only on a training data set and then used a validation set to assess the accuracy of a PCR model with 10 principle components. Below is a description of the results.

* **Optimal Number of Principle Components:** 10
* **Test RMSE PCR:** $27,601.17
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67

Unfortunately again we see that the decrease in variance from a dimension reduction method does not help our predictions. It appears that the increase in bias outweighs the decrease in variance. It appears like some of our predictors may be non-linear and our model is suffereing from too much bias. Hopefully we will have better results with methods that reduce bias.

```{r, PCR, include=FALSE, echo=FALSE}
set.seed(1)
#Find best number of principle components
pcr.fit = pcr(SalePrice ~., data = cleanData, scale = TRUE, validation = "CV")
#summary(pcr.fit)

#Validation Set Results
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train.pcr <- cleanData[sample, ]
test.pcr  <- subSetData[-sample, ]
pcr.fittrain = pcr(SalePrice ~., data = train.pcr, scale = TRUE)
pcr.pred = predict(pcr.fittrain, test.pcr, ncomp = 10)
mean((exp(pcr.pred) - exp(test.pcr$SalePrice))^2)^(1/2)
```

```{r, PCR Plots, echo=FALSE}
validationplot(pcr.fit, val.type = "MSEP")
```

#Moving Beyond Linearity
The goal of this section was to see if we could beat our Lasso prediction accuracy by fitting more complex functions to the predictors using a Generalized Additive Model and appropriate basis functions. We used the predictors that had Non-Zero coefficients in our best Lasso Model. The equation for the Generalized Additive Model is $y_i = \beta_0+\sum_{j=1}^pf_j(x_ij) + \epsilon_i$. Are hope was that by fitting non-linear functions of the predictors we can increase our models flexibility for some predictors and therefore reduce the overall bias. Unfortunately this did not help a lot. Below is a comparison of our models accuracy. We tried a lot of different basis functions for the predictors and the best cross Validation error rate we acheived was $19,787.60. Below is a graph of the predictor "Year Built" which did appear to imrpove by fitting a Natural Cubic Spline to it. Unfortunately there were not a lot of other variables that improved over their Linear Least Squares fits. 

* **Test RMSE GAM:** $19,787.60
* **Test RMSE Lasso:** $20,248.17
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67

```{r GAM setup, include=FALSE, echo=FALSE}
set.seed(1)
#Create new data from from Lasso data frame
LassoBestCoef = coef(lasso.mod, s = bestLassoLambda)
GAM.predictors = LassoBestCoef@Dimnames[[1]][which(LassoBestCoef != 0)]
#GAM.predictors
GAM.predictors = GAM.predictors[-1]
#GAM.predictors
GAM.data = cleanData[names(cleanData)[names(cleanData) %in% GAM.predictors]]
GAM.data$SalePrice = cleanData$SalePrice

##Create Train and Test Data
sample <- sample.int(n = nrow(GAM.data), size = floor(.75*nrow(GAM.data)), replace = F)
train.GAM <- GAM.data[sample, ]
test.GAM  <- GAM.data[-sample, ]
```

```{r GAM Model, include=FALSE, echo=FALSE}
#GAM Model
GAM.model = gam(SalePrice ~ ns(LotFrontage,3) +
                            ns(LotArea,3) +
                            ns(YearBuilt, 3) +
                            ns(YearRemodAdd,3) +
                            ns(BsmtFinSF1, 3) +
                            ns(TotalBsmtSF, 3) +
                            ns(GrLivArea, 3) +  
                            ns(WoodDeckSF, 3) +
                            ns(OpenPorchSF, 3) +
                            OverallQual +
                            OverallCond +
                            ExterQual +
                            BsmtQual +
                            HeatingQC +
                            CentralAir +
                            BsmtFullBath +
                            KitchenAbvGr +
                            KitchenQual +
                            Functional +
                            FireplaceQu +
                            GarageFinish +
                            GarageCars +
                            GarageQual +
                            PavedDrive +
                            MSZoning.FV +
                            MSZoning.RM +
                            Alley.Pave +
                            LotConfig.CulDSac+
                            Neighborhood.BrDale+
                            Neighborhood.BrkSide+
                            Neighborhood.ClearCr+
                            Neighborhood.Crawfor+
                            Neighborhood.Edwards+
                            Neighborhood.MeadowV+
                            Neighborhood.Mitchel+
                            Neighborhood.NridgHt+
                            Neighborhood.OldTown+
                            Neighborhood.Somerst+
                            Neighborhood.StoneBr+
                            Neighborhood.Veenker+
                            Condition1.Artery+
                            Condition1.Norm+
                            Condition1.RRAe+
                            Exterior1st.MetalSd+
                            Exterior1st.HdBoard+
                            Exterior1st.BrkFace+
                            BldgType.Twnhs+
                            BldgType.1Fam+
                            MasVnrType.Stone+
                            Foundation.BrkTil+
                            Foundation.PConc+
                            BsmtExposure.Gd+
                            BsmtExposure.No+
                            BsmtFinType1.GLQ+
                            BsmtFinType1.Unf+
                            BsmtFinType2.ALQ+
                            BsmtFinType2.BLQ+
                            Electrical.FuseA+
                            Fence.GdWo+
                            Fence.None+
                            SaleType.COD+
                            SaleType.New+
                            SaleType.WD+
                            SaleCondition.Abnorml 
                            , data = train.GAM )

#Predict and get RMSE
preds = predict(GAM.model, newdata = test.GAM)
mean((exp(preds) - exp(test.GAM$SalePrice))^2)^(1/2)
```
```{r Plot Attempt, echo=FALSE}
fit.YearBuilt = lm(SalePrice ~ ns(YearBuilt, 3), data = GAM.data)
YearBuilt.lims = range(GAM.data$YearBuilt)
YearBuilt.grid = seq(from = YearBuilt.lims[1], to = YearBuilt.lims[2])
pred = predict(fit.YearBuilt, newdata = list(YearBuilt=YearBuilt.grid), se=T)
plot(GAM.data$YearBuilt, GAM.data$SalePrice, xlab = "Year Built", ylab = "log(SalePrice)" )
lines(YearBuilt.grid, pred$fit, col ="blue", lwd=2)
lines(YearBuilt.grid, pred$fit + 2*pred$se, lty="dashed", col = "red")
lines(YearBuilt.grid, pred$fit - 2*pred$se, lty="dashed", col = "red")
```


