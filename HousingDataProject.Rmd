---
title: "Housing Data Project"
author: "Justin Sheldon, Jeremy Swiatek"
date: "October 24, 2018"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    theme: united
    highlight: tango
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(boot)
library(car)
library(plyr)
library(dummies)
library(ggplot2)
library(gridExtra)
library(leaps)
library(ggrepel)
library(ggthemes)
library(glmnet)
library(pls)
library(psych)
library(gam)
library(splines)
library(corrplot)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
```

#Introduction
Location, Location, Location, is that all you really need to know about housing to make an informed decision on the value of a house? Being an informed consumer is always a good idea but especially on the largest purchase you will probably make in your lifetime. We have two goals for this project. The first goal is to be able to predict the sale price of a house with reasonably good accuracy. The second goal is to understand how different characteristics of a house affect the overall sale pice.
 
The response variable is "Sales Price" which is just the final sale price of the house. We have 79 predictor variables (177 after dummy variable creation). There are too many predictor variables to describe them all here but we do go into more detail later in "The Data" section and when we perform subset selection. Check out "The Data" section to see a more detailed description of our response variable, predictor variables, and all the stuff we did to clean this data set and get it ready for modeling.

We used 7 different statistical learning methods that we learned in class this semester to attempt to acheive tha goals stated above. The methods used are Multiple Linear Regression, Forward Stepwise Selection, Backwards Stepwise Selection, Ridge Regression, Lasso, Principle Components Regression, and finally a Generalized Additive Metho where we combined a mix of different smoothing splines on the predictors. Our hope was that as we applied all of these different methods we would get a better feel for the data and see what works, what does not work, and why. 

Another goal of this project was to learn more about and gain experience with the different statistical learning methods we have covered in class this semester. This project is broken up into sections for each of the different methods. There is a table of contents to the left that you can click to jump to any section you want to look at. We briefly summarize our findings below.

**Brief Summary of Conclusions:**
After all the data cleaning and model fitting we were able to acheive a Test RMSE accuracy of approximately $20,000. Lasso and GAM were the most effective methods we used. The findings in terms of which characteristics of a house affect the overall sale price were very intuitive. Things like house size, quality, and year built all have a positive correlation on the sale price of a house, and living in the worst neighboorhood in the city has a negative effect on the sale price of a house. The most significant predictors were predictors related to the size of the house, the quality of the house and where the house was located.

##The Data
The dataset we will be using is the House Prices dataset from www.kaggle.com. The Response Variable is SalePrice and there are 79 explanatory variables. There are 1460 observations in total in the training Data. We did a lot of data cleaning before building any models. Below is everything we did catagorized into sections. After all the data cleaning we ended up with a training set with 1458 observations and 178 variables (after dummy variable creation).

```{r echo = FALSE}
trainData = read.csv("train.csv", stringsAsFactors = FALSE)
testData = read.csv("test.csv", stringsAsFactors = FALSE)
```

#### **A Look at the Predictors**
Here is a brief look at some of the predictors that are highly correlated with Sale Price. As you can see from the correlation matrix below intuitive things like Overall Quality, Ground Floor Living Area (in square feet), Total Basement Square Feet, and Year Built are all correlated with Sale Price. Also we can see from the correlation matrix below that we are going to have some multicollinearity issues. Garage Cars and Garage Area have a correlation of 0.88 which makes sense because obviously the bigger your garage the more cars you can fit in it. These highly correlated predictors are going to cause us problems when we build models and therefore we dealth with them in the section below called "Dealing with Multicollinearity."

```{r Correlation Plot, echo=FALSE}
numericVars <- which(sapply(trainData, is.numeric))
numericVarNames <- names(numericVars)
all_numVar <- trainData[, numericVars]
cor_numVar <- cor(all_numVar, use="pairwise.complete.obs")
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt")
```

#### **Regularization of predictors**
We converted all the numeric predictors that has a skewness rating of greater then absolute value of 1 by taking the log of them. This had the effect of removing any skewed tails and effectively normailzing the predictors.


#### **Dealing with Skewed Response Variable**

The response variable SalePrice was heavily right skewed. We fixed this with a log transformation. Below is a couple before and after graphs showing the results. We will be using the log(SalePrice) as our predictor variable in our models.

```{r Create Log Sale Price Data Frame, include=FALSE, echo=FALSE}
train.before = trainData
trainData$SalePrice = log(trainData$SalePrice)
```
```{r Plot of SalePrice vs. Normal Distribution , echo=FALSE , fig.align = "center", fig.height = 4, fig.width = 6}
p1 = ggplot(train.before, aes(x=SalePrice)) +
        geom_histogram(aes(y = ..density..), fill="#00468B", binwidth = 10000) +
        stat_function(fun = dnorm, args = list(mean(train.before$SalePrice),sd(train.before$SalePrice)), colour = "red", lwd = 1) +
        scale_x_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000")) +
        labs(title = "SalePrice vs. Normal Distribution")

p2 = ggplot(trainData, aes(x=SalePrice)) +
        geom_histogram(aes(y = ..density..), fill="#00468B", binwidth = .15) +
        stat_function(fun = dnorm, args = list(mean(trainData$SalePrice),sd(trainData$SalePrice)), colour = "red", lwd = 1) +
        scale_x_continuous(breaks= seq(10, 14, by=.5)) +
        labs(title = "Log SalePrice vs. Normal Distribution")

grid.arrange(p1, p2, nrow = 1, ncol = 2)
```
```{r QQ Plots, echo=FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
#QQ PLots
par(mfrow = c(1,2))
qqnorm(train.before$SalePrice, main = "Sale Price QQ Plot")
qqline(train.before$SalePrice)
qqnorm(trainData$SalePrice, main = "Log Sale Price QQ Plot")
qqline(trainData$SalePrice)
```

####  **Dealing with NA's**
Upon further inspection of the data we noticed we had some variables that were causing us trouble. There are 19 variables with missing data. For a lot of the categorical variables "NA's" really meant "None" so we fixed those by converting NA level to a "None" level. There was also a couple of observations that had one missing value for a given variable and we just fixed that by predicting the mean/mode of that variable for that observation. Any other additional modification is listed below along with the reason why we choose that modification.

* **LotFrontage:** Linear Feet of street connected to property. This one was a tough decision. It's missing 259/1460 approximately 18% of the data for this variable but we didnt want to lose the variable entirely. We decided to fill in the NA's with the mean value of the variable.
* **GarageYrBlt:** Garage Year Built. We replaced the NA's with the Year the house was built as long as it actualy had a garage. We expect GarageYrBlt and YearBuilt to be highly correlated and eventually one will probably have to be removed from the model anyways.
* **MiscFeature:** Miscellaneous feature. This variable was essentially worthless. Almost all of the data was missing and the data that did exist was 'tennis court' and 'other'. We removed this variable.

```{r, Finding the NAs, include=  FALSE, comment = ""}
NAVariables <- which(colSums(is.na(trainData)) > 0)
sort(colSums(sapply(trainData[NAVariables], is.na)), decreasing = TRUE)
```
```{r, Fixing NA Variables, echo=FALSE, include=FALSE}
#Fixing Pool QC
trainData$PoolQC[is.na(trainData$PoolQC)] = 'None'
Qualities = c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)
trainData$PoolQC = as.integer(revalue(trainData$PoolQC, Qualities))

#Fixing Alley
trainData$Alley[is.na(trainData$Alley)] = 'None'
trainData$Alley = as.factor(trainData$Alley)

#Fixing Fence
trainData$Fence[is.na(trainData$Fence)] = 'None'
trainData$Fence = as.factor(trainData$Fence)

#Fixing Fireplace
trainData$FireplaceQu[is.na(trainData$FireplaceQu)] = 'None'
trainData$FireplaceQu = as.integer(revalue(trainData$FireplaceQu, Qualities))

#Fixing LotFrontage and other lot variables
for (i in 1:nrow(trainData)){
        if(is.na(trainData$LotFrontage[i])){
               trainData$LotFrontage[i] = as.integer(mean(trainData$LotFrontage, na.rm = TRUE))
        }
}
trainData$LotShape = as.integer(revalue(trainData$LotShape, c('IR3'=0, 'IR2'=1, 'IR1'=2, 'Reg'=3)))
trainData$LotConfig = as.factor(trainData$LotConfig)

#Fixing Garage variables
#Year Built
trainData$GarageYrBlt[is.na(trainData$GarageYrBlt)] = trainData$YearBuilt[is.na(trainData$GarageYrBlt)]
#Garage Type
trainData$GarageType[is.na(trainData$GarageType)] = 'None'
trainData$GarageType = as.factor(trainData$GarageType)
#Garage Finish
trainData$GarageFinish[is.na(trainData$GarageFinish)] = 'None'
FinishLevels = c('None'=0, 'Unf'=1, 'RFn'=2, 'Fin'=3)
trainData$GarageFinish<-as.integer(revalue(trainData$GarageFinish, FinishLevels))
#Garage Quality
trainData$GarageQual[is.na(trainData$GarageQual)] = 'None'
trainData$GarageQual = as.integer(revalue(trainData$GarageQual, Qualities))
#Garage Condition
trainData$GarageCond[is.na(trainData$GarageCond)] = 'None'
trainData$GarageCond = as.integer(revalue(trainData$GarageCond, Qualities))

#Fixing Basement Variables
#Additional NA in these two observations
trainData[!is.na(trainData$BsmtFinType1) & (is.na(trainData$BsmtCond)|is.na(trainData$BsmtQual)|is.na(trainData$BsmtExposure)|is.na(trainData$BsmtFinType2)), c('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')]
#Predict Most common occurence to fix these two observations
trainData$BsmtFinType2[333] = "Unf"
trainData$BsmtExposure[949] = "No"
#Basement Quality
trainData$BsmtQual[is.na(trainData$BsmtQual)] = 'None'
trainData$BsmtQual = as.integer(revalue(trainData$BsmtQual, Qualities))
#Basement Condition
trainData$BsmtCond[is.na(trainData$BsmtCond)] = 'None'
trainData$BsmtCond = as.integer(revalue(trainData$BsmtCond, Qualities))
#Basement Exposure
trainData$BsmtExposure[is.na(trainData$BsmtExposure)] = 'None'
trainData$BsmtExposure = as.factor(trainData$BsmtExposure)
#Basement Finish Type 1
trainData$BsmtFinType1[is.na(trainData$BsmtFinType1)] = 'None'
trainData$BsmtFinType1 = as.factor(trainData$BsmtFinType1)
#Basement Finish Type 2
trainData$BsmtFinType2[is.na(trainData$BsmtFinType2)] = 'None'
trainData$BsmtFinType2 = as.factor(trainData$BsmtFinType2)

#Fixing Masonry Variables
#Masonary Veneer Type
trainData$MasVnrType[is.na(trainData$MasVnrType)] = 'None'
trainData$MasVnrType = as.factor(trainData$MasVnrType)
#Masonary Veneer Area
trainData$MasVnrArea[is.na(trainData$MasVnrArea)] = 0

#Fixing Electrical
trainData$Electrical[is.na(trainData$Electrical)] = "SBrkr"
trainData$Electrical = as.factor(trainData$Electrical)


#Fixing MiscFeature (junk just remove)
trainData = subset(trainData, select = -MiscFeature)
```
```{r, Fixing Character Variables, echo=FALSE,include=FALSE}
#Turning Char variables into factor variables
trainData$MSZoning = as.factor(trainData$MSZoning)
trainData$Street = as.factor(trainData$Street)
trainData$LandContour = as.factor(trainData$LandContour)
trainData$Neighborhood = as.factor(trainData$Neighborhood)
trainData$Condition1 = as.factor(trainData$Condition1)
trainData$BldgType = as.factor(trainData$BldgType)
trainData$HouseStyle = as.factor(trainData$HouseStyle)
trainData$RoofStyle = as.factor(trainData$RoofStyle)
trainData$Exterior1st = as.factor(trainData$Exterior1st)
trainData$Exterior2nd = as.factor(trainData$Exterior2nd)
trainData$Foundation = as.factor(trainData$Foundation)
trainData$Heating = as.factor(trainData$Heating)
trainData$SaleType = as.factor(trainData$SaleType)
trainData$SaleCondition = as.factor(trainData$SaleCondition)


#Turning Char Variables into Ordinal Numeric
trainData$LandSlope = as.integer(revalue(trainData$LandSlope, c('Sev'=0, 'Mod'=1, 'Gtl'=2)))
trainData$ExterQual = as.integer(revalue(trainData$ExterQual, Qualities))
trainData$ExterCond = as.integer(revalue(trainData$ExterCond, Qualities))
trainData$HeatingQC = as.integer(revalue(trainData$HeatingQC, Qualities))
trainData$CentralAir = as.integer(revalue(trainData$CentralAir, c('N'=0, 'Y'=1)))
trainData$KitchenQual = as.integer(revalue(trainData$KitchenQual, Qualities))
trainData$Functional = as.integer(revalue(trainData$Functional, c('Sal'=0, 'Sev'=1, 'Maj2'=2, 'Maj1'=3, 'Mod'=4, 'Min2'=5, 'Min1'=6, 'Typ'=7)))
trainData$PavedDrive = as.integer(revalue(trainData$PavedDrive, c('N'=0, 'P'=1, 'Y'=2)))

#Not a part of the data just observation #
trainData = subset(trainData, select = -Id)

#Make sure Empty
charVariables = names(trainData[,sapply(trainData, is.character)])
charVariables
```

####  **Dealing with Multicollinearity**
We decided to just deal with multicollinearity issues before building any models. We checked the correlation matrix for all numeric variables and decided to remove any pair of variables with a correlation of 0.75 or higher. Of the two variables we kept Whichever variable had a higher correlation with SalePrice. Below is a list of all of these pairs and which variables we kept.

* **Garage Yr Blt vs. Year Built:** correlation = 0.845. We kept Year Built
* **Garage Area vs. Garage Cars:** correlation = 0.882. We kept Garage Cars
* **Total Basement SqFt vs. 1st Floor SqFt** correlation = 0.819. We kept Total Basement SqFt
* **Garage Condition vs. Garage Quality** correlation = 0.959. We kept Garage Quality
* **Total Rooms Above Ground vs. Ground Living Area** correlation = 0.825. We kept Ground Living Area

```{r Multicollinearity, include=FALSE, echo=FALSE}
#Finding the Correlated Variables
    # numericVars <- which(sapply(trainData, is.numeric))
    # all_numVar <- trainData[, numericVars]
    # #Garage Year Build vs YearBuilt
    # print(cor(all_numVar, trainData$GarageYrBlt) > 0.75)
    # print(cor(trainData$GarageYrBlt, trainData$SalePrice))
    # print(cor(trainData$YearBuilt, trainData$SalePrice))
    # #Garage Garage Cars vs GarageArea
    # print(cor(trainData$GarageArea, trainData$GarageCars))
    # print(cor(trainData$GarageArea, trainData$SalePrice))
    # print(cor(trainData$GarageCars, trainData$SalePrice))
    # #TotalBsmtSF vs. X1stFlrSF
    # print(cor(trainData$TotalBsmtSF, all_numVar) > 0.75)
    # print(cor(trainData$TotalBsmtSF, trainData$X1stFlrSF))
    # print(cor(trainData$TotalBsmtSF, trainData$SalePrice))
    # print(cor(trainData$X1stFlrSF, trainData$SalePrice))
    # #GarageCond vs. Garage Quality
    # print(cor(trainData$GarageCond, all_numVar) > 0.75)
    # print(cor(trainData$GarageCond, trainData$GarageQual))
    # print(cor(trainData$GarageCond, trainData$SalePrice))
    # print(cor(trainData$GarageQual, trainData$SalePrice))
    # #Total Rooms Above Ground vs. Ground Living Area
    # print(cor(trainData$TotRmsAbvGrd, all_numVar) > 0.75)
    # print(cor(trainData$TotRmsAbvGrd, trainData$GrLivArea))
    # print(cor(trainData$TotRmsAbvGrd, trainData$SalePrice))
    # print(cor(trainData$GrLivArea, trainData$SalePrice))
    
#Remove the Variables
trainData = subset(trainData, select = -GarageYrBlt)
trainData = subset(trainData, select = -GarageArea)
trainData = subset(trainData, select = -X1stFlrSF)
trainData = subset(trainData, select = -GarageCond)
trainData = subset(trainData, select = -TotRmsAbvGrd)

#Check Removed
#dim(trainData)
```

####  **Dealing with Sparse Data**
Some of our factor variables had levels with a count of less then 5. This was causing problems with using cross validation. Below is a list of all the variables that had this issue and how we dealth with them.

* **Utilities:** Type of utilities available. 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Condition2:** Proximity to various conditions. 8 levels with 99% of the data belonging to one level. We removed this variable.
* **RoofMatl1:** Roof Material. 8 levels with more than 98% of the data belonging to one level. We removed this variable.
* **Street:** 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Heating:** 6 levels with 98% of the data belonging to one level. We removed this variable.

We kept the rest of the variables that had sparse levels in the model because they had other levels that were not sparse and could be important in future models. We converted all the factor variables into dummy variables and then just removed the dummy variables with levels that had less than 10 observations. This ended up being 32 out of 162 levels.

```{r Looking at Catagorical Variables, echo=FALSE, include=FALSE}
#Find Categorical Variables
numericVars = which(sapply(trainData, is.numeric))
all_factorVars = trainData[,-numericVars]
print(names(all_factorVars))
```
```{r Remove Worthless Variables, include=FALSE,echo=FALSE}
#Too Sparse < 2% of data not in same category
trainData = subset(trainData, select = -Utilities)
trainData = subset(trainData, select = -Condition2)
trainData = subset(trainData, select = -RoofMatl)
trainData = subset(trainData, select = -Street)
trainData = subset(trainData, select = -Heating)
```
```{r Dummy Variables and new Train DF, include=FALSE, echo=FALSE}
#Find all the factor variables
factorVars = which(sapply(trainData, is.factor))
factorVars = names(factorVars)
#Create seperate data frame to convert factor variables to dummy varaibles
DFfactors = trainData[, factorVars]
DFdummies = dummy.data.frame(DFfactors, sep = ".")
#Find Sparse dummy variables
sparseLevels <- which(colSums(DFdummies[1:nrow(trainData[!is.na(trainData$SalePrice),]),])<10)
#Remove Sparse dummy variables
DFdummies = DFdummies[,-sparseLevels]
#combine all the non factor variables in original data frame with our new dummy variables
DFnumeric = trainData[,!(names(trainData) %in% factorVars)]
trainDF.dummies = cbind(DFnumeric, DFdummies)
```

#### **Dealing with Outliers**
The only outliers we found were observations 524 and 1299. We deemed these outliers significant enough to remove because they were huge houses with a overall quality rating of 10 (the highest rating) and yet they sold for a very low price. By looking at the size and quality rating of the house if appears they may have missed a 0 on the end of the price. Size of the house (GrLiveArea) and Overall Quality are the two highest positive correlated variables with SalePrice which lead us to believe these SalePrices were a mistake and we removed these observations from the data set. We left the graphs below in terms of the actual Sale Price and not log of the Sale Price so it would be easier to interpret

```{r Finding Outliers, echo=FALSE, fig.align = "center", fig.height = 3.5, fig.width = 6, comment=""}
ggplot(data=train.before[!is.na(train.before$SalePrice),], aes(x=GrLivArea, y=SalePrice))+
        geom_point(col='#00468B') + geom_smooth(method = "lm", se=FALSE, color="red", aes(group=1)) +
        scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000") ) +
        geom_text_repel(aes(label = ifelse(trainData$GrLivArea[!is.na(trainData$SalePrice)]>4500, rownames(trainData), '')))
train.before[c(524, 1299), c('SalePrice', 'GrLivArea', 'OverallQual')]
```

```{r final Data to be used for models, echo=FALSE, include=FALSE}
#Use cleanData for all future models
cleanData = trainDF.dummies
cleanData = cleanData[-c(524,1299),]
```

#                                                   Methods Section


##Multiple Linear Regression
The main purpose of building a Multiple Linear Regression model is to use it as a benchmark that we compare all future models to. Below is a brief summary of our Multiple Linear Regression model that includes all variables in the model fit. The model euqation will be in the form $Y = \beta_0 +\sum_{i=1}^p \beta_i X_i + \epsilon$. This is just the general form the equation will be in. There are 177 predictors and coefficients so we did not write out the entire model equation here. Also we cover the model assumptions and the potential problems this model may have below.

**Model Accuracy**
*We change log(SalePrice) back into Sale Price so that it is easier to interpret the results.


* **10 Fold Cross Valiation RMSE:** $21,767.80
* **Adjusted R-Squared:** 0.9311
* **Test RMSE Null Model:** $79,513.67


**Most Significant Variables**

* **OverallQual:** Rates the overall material and finish of the house.
* **OverallCond:** Rates the overall condition of the house.
* **YearBuilt:** Original Construction Date.
* **LotArea:** Lot size in Square Feet.
* **GarageCars:** Size of Garage in car capacity
* **HeatingQC:** Heating Quality and Condition Rating.
* **Neighborhood.MeadowV :** Neighboorhood Meadow Village
* **BsmtUnfSF :** Unfinished square feet of basement area
* **GrLivArea :** Above Ground Living Area in Square Feet.



```{r, Base Model, echo=FALSE, include=FALSE}
#Just use this model for Potential Problems Section
set.seed(88)
sample <- sample.int(n = nrow(cleanData), size = floor(.75*nrow(cleanData)), replace = F)
train.mlr <- cleanData[sample, ]
test.mlr  <- cleanData[-sample, ]
baseModel = lm(SalePrice ~ ., data = train.mlr)
mlr.pred = predict(baseModel, newdata = test.mlr)
mean((exp(mlr.pred) - exp(test.mlr$SalePrice))^2)^(1/2)
```

###  Model Assumptions
Below we check the Multiple Linear Regression model assumptions. We also checked these assumptions again on future models but to avoid unnecessary duplication (and boring the reader) we left those parts out of the project.

##### **Linearity and Constant Variance Assumptions**
From the Residual vs. fitted values plot it appears as if a linearity and constant variance assumptions are valid. I do not see a Non-Linear trend in the residuals and nothing to be too concerned about here. Later on in the project when we tried to improve prediction accuracy by fitting Non-linear functions to the predictors in a GAM we noticed that indeed a lot of the predictors do have a linear realtionship with Sale Price. (This may be part of the reason our Multiple Linear Regression model performs so well). Also we already transformed our Response variable which helped with constant variance.

```{r, Model Residuals, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6}
plot(baseModel$fitted.values, baseModel$residuals, ylab = "Residuals", xlab= "Fitted Values", col = "#00468B")
```

##### **Correlation of Error Terms**
We do not see any noticable trend present in the error terms. I think it is fine to assume they are uncorrelated and we can trust the standard error and p-value results we obtained in the model. Note that this is only a fraction of the observations. The graphic for all the observations was overly cluttered and hard to interpret. This same trend extends to all the observations. 

```{r Correlation of Error Terms,echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6}
plot(c(1:250), baseModel$residuals[1:250], xlab = "Observations", ylab = "Residuals", col = "azure4")
lines(c(1:250), baseModel$residuals[1:250], col = "#00468B")
```

#####     **Outliers and High Leverage Points**
After looking at the Studentized Residuals vs. Leverage plots we do not see any points that have significantly high leverage while also having a large residual value. This leads us to beleive there is not any one observation that is significantly effecting our model fit and accuracy. There are observations with high leverage or high residuals but as long as they do not occur together we are fine to leave these observations in the model.

```{r, Model Outliers, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6}
student.residuals = sort(rstudent(baseModel), decreasing = TRUE)
Leverage = hat(model.matrix(baseModel))
plot(Leverage ,student.residuals, xlab = "Leverage", ylab = "Studentized Residuals", col="#00468B")
```



##                                      Model Selection and Regularization

###         Subset Selection
The goal of subset Selection is to try and find the smallest subset of the predictors that are statistically significant and still lead to a good model fit. The advantage to an approach like this over Multiple Linear Regression is that using a subset of the original predictors can lead to a significant reduction in variance while only slightly increasing the bias and therefore lead to an improvement in prediction accuracy. Subset Selection still uses Least Squares fits with the final model equation in the form of  $Y = \beta_0 +\sum_{i=1}^p \beta_i X_i + \epsilon$.  Our hope is that a subset selection model will out perform the full model we used in Multiple Linear Regression Section. Below we perform both Forward stepwise Selection and Backwards Stepwise Selection and give more details about the methods used and the results.

```{r Stuff We Need for Subset Selection, include=FALSE, echo=FALSE}
subSetData = cleanData
```

#### **Forward Stepwise Selection**
To perform forward stepwise selection we fit a model that iteratively steps forward adding the next most significant predictor at each iteration. We then used Cross Validation on each model from 1-177 predictors to find the best model size for our data. We also looked at the CP, BIC and Adjusted R^2 scores to decide on the best model size. Below is graphs of all 4 methods we used in picking the best model size.

```{r, Forward Stepwise Selection, include=FALSE, echo=FALSE}
#CP, BIC, AdjR^2
regfit.fwd = regsubsets(SalePrice ~., data = subSetData, nvmax = 160, method = "forward")
regfit.fwd.summary = summary(regfit.fwd)

#validation set 
set.seed(1)
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train <- subSetData[sample, ]
test  <- subSetData[-sample, ]

regfit.valid = regsubsets(SalePrice ~., data = train, nvmax = 160, method = "forward")
test.matrix = model.matrix(SalePrice ~., data = test)
validation.errors = rep(NA, 160)
for (i in 1:160){
    coefi = coef(regfit.valid, id=i)
    pred = test.matrix[,names(coefi)]%*%coefi
    validation.errors[i] = mean((test$SalePrice-pred)^2)
}

#Get Test RMSE on Sale Price
coefi2 = coef(regfit.valid, id=50)
pred2 = test.matrix[,names(coefi2)]%*%coefi2
mean(( exp(test$SalePrice) - exp(pred2) )^2)^(1/2)
```

```{r plots to include, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6}
#set xlab abd ylab and points
par(mfrow = c(2,2))
plot(regfit.fwd.summary$adjr2, type = "l", xlab = "Number of Variables", ylab = "Adjusted R Squared")
points(which.max(regfit.fwd.summary$adjr2), regfit.fwd.summary$adjr2[which.max(regfit.fwd.summary$adjr2)], col="red", cex=2,pch=20)
plot(regfit.fwd.summary$bic, type = "l", ylab = "BIC", xlab = "Number of Variables")
points(which.min(regfit.fwd.summary$bic), regfit.fwd.summary$bic[which.min(regfit.fwd.summary$bic)], col="red", cex=2,pch=20)
plot(regfit.fwd.summary$cp, type = "l", ylab = "CP", xlab = "Number of Variables")
points(which.min(regfit.fwd.summary$cp), regfit.fwd.summary$cp[which.min(regfit.fwd.summary$cp)], col="red", cex=2,pch=20)
plot(validation.errors, type = "l", ylab = "Validation MSE", xlab = "Number of Variables")
points(which.min(validation.errors), validation.errors[which.min(validation.errors)], col="red",cex=2,pch=20)
```

Forward stepwise Selection revealed that the best model using this method is somewhere between 30-50 variables in the model. Remember this is not necessarly the best model because forward stepwise can get stuck in a suboptimal path. That being said it appears like around 40 variables will turn out to be a good model. Below is a report of the model with 40 variables vs the full Least Squares Model. We tried all the different model sizes in this range and they all ended up around the same Test RMSE.

**Model Accuracy**

* **Best Model Size:** 40
* **Test RMSE FWD Stepwise Selction:** $22,584.54
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67

This model did not outperform the full Multiple Linear Regression model but it performs more or less the same. The benefit we gained over Multiple Linear Regerssion is that this model is easier to interpret while still having a similar prediction accuracy.

The most interesting thing we found from doing forward stepwise was just determining which variables were selected first and therefore are potentially the most important variables in predicting sale price.

**First 10 Variables Selected**

* **OverallQual:** Rates the overall material and finish of the house.
* **BsmtFullBath:** Number of full bathrooms in the basement.
* **YearBuilt:** Original Construction Date.
* **OverallCond:** Rates the overall condition of the house.
* **GarageQual:** Rates the Quality of the Garage.
* **HeatingQC:** Heating Quality and Condition Rating.
* **Alley.none:** Dummy Variable representing no alleyway access to the house.
* **BsmtHalfBath:** Number of half bathrooms in the basement.
* **GarageFinish:** Rating of Interior finish of the garage.
* **MSZoning.FV:** Dummy Variable for Zoning Code for "Floating Village Residential"


#### **Backwards Stepwise Selection**
The goal, methods and equation for backwards stepwise selection is the same as that of forward stepwise selection so we will not restate them all here.

```{r, Backwards Stepwise Selection, include=FALSE, echo=FALSE}
#CP, BIC, AdjR^2
regfit.back = regsubsets(SalePrice ~., data = subSetData, nvmax = 160, method = "backward")
regfit.back.summary = summary(regfit.back)

#validation set 
set.seed(1)
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train <- subSetData[sample, ]
test  <- subSetData[-sample, ]

regfit.valid = regsubsets(SalePrice ~., data = train, nvmax = 160, method = "backward")
test.matrix = model.matrix(SalePrice ~., data = test)
validation.errors = rep(NA, 160)
for (i in 1:160){
    coefi = coef(regfit.valid, id=i)
    pred = test.matrix[,names(coefi)]%*%coefi
    validation.errors[i] = mean((test$SalePrice-pred)^2)
}
coefi2 = coef(regfit.valid, id=40)
pred2 = test.matrix[,names(coefi2)]%*%coefi2
mean(( exp(test$SalePrice) - exp(pred2) )^2)^(1/2)

```
```{r plots for backstep, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6}
par(mfrow = c(2,2))
plot(regfit.back.summary$adjr2, type = "l", xlab = "Number of Variables", ylab = "Adjusted R Squared")
points(which.max(regfit.back.summary$adjr2), regfit.back.summary$adjr2[which.max(regfit.back.summary$adjr2)], col="red", cex=2,pch=20)
plot(regfit.back.summary$bic, type = "l", ylab = "BIC", xlab = "Number of Variables")
points(which.min(regfit.back.summary$bic), regfit.back.summary$bic[which.min(regfit.back.summary$bic)], col="red", cex=2,pch=20)
plot(regfit.back.summary$cp, type = "l", ylab = "CP", xlab = "Number of Variables")
points(which.min(regfit.back.summary$cp), regfit.back.summary$cp[which.min(regfit.back.summary$cp)], col="red", cex=2,pch=20)
plot(validation.errors, type = "l", ylab = "Validation MSE", xlab = "Number of Variables")
points(which.min(validation.errors), validation.errors[which.min(validation.errors)], col="red",cex=2,pch=20)
```

Backwards Stepwise Selection ends up with pretty much the same results as forward stepwise selection. Somewhere between 30-50 variables seems to be when the graphs begin to flatten out. We decided to build a model with 40 predictors and use a validation set to estimate the RMSE. Below are the results.

**Model Accuracy**

* **Best Model Size:** 40
* **Test RMSE Backwards Stepwise Selction:** $23,581.42
* **Test RMSE FWD Stepwise Selction:** $22,584.54
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67

This models is very close to Forward Stepwise Selection but still does not outperform the Full Least Squares model. It appears like the reduction in varaiance we gain from fitting a simpler model do not outweigh the increase in bias. We see again that we can get similar results in prediction accuracy using a subset of the original predictors, but do not seem to be able to beat that baseline accuracy.

**Last 10 Variables left in model**

* **OverallQual:** Rates the overall material and finish of the house.
* **BsmtFullBath:** Number of full bathrooms in the basement.
* **YearBuilt:** Original Construction Date.
* **BsmtFinSF1:** How many square feet of the basement that is finished.
* **OverallCond:** Rates the overall condition of the house.
* **GarageQual:** Rates the Quality of the Garage.
* **BsmtUnfSF:** Unfinished square feet of basement area.
* **LotArea:** Lot Size in Square Feet
* **BsmtFinSF2:** Rating of basment finished area.
* **Neighborhood.MeadowV:** Neighboorhood location Meadow Village

#### **Interpreting the Results of Subset Selection**
This is a breif summary of the results from the subset selection models. We were not able to beat the results of our baseline model but we were able to match them with a subset of the original predictors. It is interesting to note that both subset selection methods find that having a finished basement with bathrooms seems to be important factor in predicting SalePrice (after accounting for Overall Quality). Also we see that a neighborhood dummy variable (MeadowV) appeared in the top 10. Honestly we expected to see a lot more neighborhood dummy variables so we took a deeper look at the neighborhood variable. The coefficient for this neighborhood is negative and as you can see from the plot below Meadown Village has the lowest median sale price of all neighborhoods therefore this coefficient makes sense. Also it makes sense that Meadow Village would be the first significant neighborhood selected as it appears to be one of the worst neighborhoods in terms of sales price in Ames Iowa.

```{r a look at Neighborhoods, echo=FALSE, fig.align = "center"}
neighbor.plot = ggplot(train.before, aes(Neighborhood, SalePrice)) +
                geom_boxplot(aes(fill = factor(Neighborhood)))  +
                theme(axis.text.x = element_text(angle = 65, vjust = 0.6)) +
                labs( x = "Neighborhoods", y = "Sale Price") +
                theme(legend.position="none") +
                scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000") ) +
                geom_hline(yintercept = 88000, linetype="dashed", color = "red")
neighbor.plot
```

###                 Shrinkage Methods

#### **Ridge Regression**
The goal of Ridge Regression is to find a set of coefficients that minimize  $RSS +\lambda\sum_{j=1}^p \beta^2_j$.  The summation terms works as a shrinkage penalty that incentivises the model to srink the coefficients towards 0. The hope is that by doing this we will significantly reduce model variance while only slightly increasing model bias and therefore be able to outperform the full Least Squares model. 

The equation aboves shrinkage penalty depends on the value of lambda you choose. In order to find the best value for lambda we performed cross validation on a lot of different lambda values. Below is a graph of the coefficients for different levels of lambda along with the best lambda we found using Cross Validation. (note: the function we use to fit the models automatically performs variable regularization)

```{r, Ridge Regression, include=FALSE, echo=FALSE}
#Setup
set.seed(1)
x = model.matrix(SalePrice ~., cleanData)[,-1]
y = cleanData$SalePrice
train = sample(1:nrow(x), nrow(x)/1.5)
test = (-train)
y.test= y[test]
ridge.mod = glmnet(x[train,],y[train],alpha = 0)

#Find best lamda value
cv.out = cv.glmnet(x[train,],y[train], alpha = 0)
bestLambda = cv.out$lambda.min
bestLambda

#test RMSE for best lambda
ridge.pred = predict(ridge.mod, s = bestLambda, newx = x[test,])
(mean((exp(ridge.pred)-exp(y.test))^2))^(1/2)

#baseline null model predictions
mean(( mean(exp(y[train])) -exp(y.test) )^2)^(1/2)
```
```{r, Ridge Regression plot, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 7}
plot(ridge.mod, xvar="lambda")
legend(1.75, -.125, legend=c("Best Lambda", "MSZoning.C (all)", "Neighborhood.MeadowV", "Exterior1st.BrkFace", "Neighborhood.Crawfor", "Neighborhood.StoneBr"),
       col=c("red","magenta", "black", "red", "green", 69), lty=c(2,1,1,1,1,1), cex=1)
abline(v = log(bestLambda), col = "red", lwd = 2, lty = 2)
```

The best lambda we found is close to 0 and therefore it looks like this model will be close to the full Least Squares model. This is not a huge surprise because we noticed in our subset selection models that the Test MSE kept slightly decreasing as we added more variables. Also our sample size is a lot larger then the number of predictors 1458 >> 177 so reducing variance may not be that helpful for us and the increase in bias from shrinking the coefficients may be a bigger problem. Below is a comparison in the model accuracies.

**Model Accuracy**

* **Best Lambda:** 0.044
* **Test RMSE Ridge:** $21,646.23
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67

The Ridge Regression model and the least squares model perform pretty much identically on a validation set at lamda = 0.044. They both significantly beat the Null Model that only uses the mean of Sale Price to predict Sale Price but essentially they are pretty much the same model.

#### **Lasso Regression**
The goal of Lasso Regression is similar to Ridge Regression but in Lasso Regression we also perform subset selection. This has the benefit of potentially reducing variance and therefore improving prediction accuracy while also improving interpretibilty. We hope this model will outperform the full Least Squares while also being a lot more interpretible than Ridge Regression. The equation we use to do this is similar to Ridge Regression but now we minimize $RSS +\lambda\sum_{j=1}^p\lvert \beta_j \rvert$. Again we need to find the best lambda and we use the same Cross Validation methods as before.


```{r, Lasso, include=FALSE, echo=FALSE}
#Use train and test from before 

#Baseline Lasso Model
lasso.mod = glmnet(x[train,], y[train], alpha = 1)

#Find best Lambda
cv.lasso.out = cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.lasso.out)
bestLassoLambda = cv.lasso.out$lambda.min
bestLassoLambda

#Lasso Predictions accuracy
lasso.pred = predict(lasso.mod, s = bestLassoLambda, newx = x[test,])
(mean((exp(lasso.pred)-exp(y.test))^2))^(1/2)

#Number of Non Zero coefficients at best lambda
lasso.coef = predict(lasso.mod, type = "coefficients", s=bestLassoLambda)
sum(lasso.coef!=0)
```

```{r, Lasso Regression plot, echo=FALSE, fig.align = "center", fig.height = 5, fig.width = 7}
plot(lasso.mod, xvar="lambda")
legend(-4, -.275, legend=c("Best Lambda", "MSZoning.C (all)", "Neighborhood.MeadowV", "Neighborhood.Crawfor"), col=c("red", 69, "black", "blue" ), lty=c(2,1,1,1), pt.cex=1,  cex = 0.75)
abline(v = log(bestLassoLambda), col = "red", lwd = 2, lty = 2)
```

The Lasso model with the lowest cross validation error rate is a model with 71 predictors. This is close to we saw in forward and backward stepwise selection. 

**Model Accuracy**

* **Best Lambda:** 0.0039
* **Test RMSE Lasso:** $20,248.17
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67
* **Count of Non-Zero Coefficients:** 71

This model did outperform all of our previous models (although not by that much) and has the added benefit of subset selection on the predictors. This helps with interpretibility because of having fewer variables. Currently this is the best model for both prediction and interpretibility we have.

```{r Coefficient Plot, echo=FALSE, include=FALSE}
# ##Maybe Do not do this!!!!
# LassoBestCoef = coef(lasso.mod, s = bestLassoLambda)
# Lasso.coef = LassoBestCoef@x
# Lasso.coef.values = sort(abs(Lasso.coef[which(Lasso.coef != 0)]), decreasing = TRUE)
# Lasso.coef.values = Lasso.coef.values[2:11]
# Lasso.coef.values[1] = -1*Lasso.coef.values[1]
# #Get top 10 names
# Lasso.coef.names = c("MSZoning.C", "Test", "test2","test3", "Test4", "test5","test6","Test7", "test8","test9")
# #Make a better plot using ggplot like above (curved x axis, colors)
# plot(as.factor(Lasso.coef.names), Lasso.coef.values)
```

###             Dimension Reduction Methods
#### **Principle Components Regression**
The goals of principle components regression is to find principle components of the form  Z = $\sum_{i=1}^n \phi_jX_j$  such that a few principle components can be used to predict the response. The hope is that this will lead to a substantial reduction in variance while only causing a slight increase in bias. Note that each principle component is a linear cobination of all the predictors. We want this method to outperform the Least Squares. This has an advantage over Least Squares in that it will have a lower variance but it will have a higher bias because we are assuming a simpler model. 

We fit a Principle Components Regression model (with scale = TRUE to regularize the data) and used cross validation to determine the optimal number of principal components. Below is a plot of this model fit. After finding the optimal number of components we fit a new model only on a training data set and then used a validation set to assess the accuracy of a PCR model with 10 principle components. Below is a description of the results.

**Model Accuracy**

* **Optimal Number of Principle Components:** 10
* **Test RMSE PCR:** $27,601.17
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67

Unfortunately again we see that the decrease in variance from a dimension reduction method does not help our predictions. It appears that the increase in bias outweighs the decrease in variance. It appears like some of our predictors may be non-linear and our model is suffereing from too much bias. Hopefully we will have better results with methods that reduce bias.

```{r, PCR, include=FALSE, echo=FALSE}
set.seed(1)
#Find best number of principle components
pcr.fit = pcr(SalePrice ~., data = cleanData, scale = TRUE, validation = "CV")
#summary(pcr.fit)

#Validation Set Results
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train.pcr <- cleanData[sample, ]
test.pcr  <- subSetData[-sample, ]
pcr.fittrain = pcr(SalePrice ~., data = train.pcr, scale = TRUE)
pcr.pred = predict(pcr.fittrain, test.pcr, ncomp = 10)
mean((exp(pcr.pred) - exp(test.pcr$SalePrice))^2)^(1/2)
```

```{r, PCR Plots, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6}
validationplot(pcr.fit, val.type = "MSEP", main = "Finding Optimal Number of Principal Components")
```

##                  Moving Beyond Linearity
The goal of this section was to see if we could beat our Lasso prediction accuracy by fitting more complex functions to the predictors using a Generalized Additive Model and appropriate basis functions. We used the predictors that had Non-Zero coefficients in our best Lasso Model. The equation for the Generalized Additive Model is $y_i = \beta_0+\sum_{j=1}^pf_j(x_ij) + \epsilon_i$. Are hope was that by fitting non-linear functions of the predictors we can increase our models flexibility for some predictors and therefore reduce the overall bias while not increasing variance too much. This helped a little but not a lot. Below is a comparison of our models accuracy. We tried a lot of different basis functions for the predictors and the best cross Validation error rate we acheived was $19,787.60. Below is a graph of a predictor "Year Built" which did appear to imrpove by fitting a Natural Cubic Spline to it. Unfortunately there were not a lot of other variables that improved over their Linear Least Squares fits.

**Model Accuracy**

* **Test RMSE GAM:** $19,787.60
* **Test RMSE Lasso:** $20,248.17
* **Test RMSE Full MLR:** $21,767.80
* **Test RMSE Null Model:** $79,513.67

For the benefit of interpretibility we decided to try to build a simpler GAM and see if we could narrow down the number of predictors and not loose a statistically significant amount of prediction accuracy. We ran a partial F-test to make sure our reduced model was sufficient (p-value = 2.2e-16). 

```{r GAM setup, include=FALSE, echo=FALSE}
set.seed(1)
#Create new data from from Lasso data frame
LassoBestCoef = coef(lasso.mod, s = bestLassoLambda)
GAM.predictors = LassoBestCoef@Dimnames[[1]][which(LassoBestCoef != 0)]
#GAM.predictors
GAM.predictors = GAM.predictors[-1]
#GAM.predictors
GAM.data = cleanData[names(cleanData)[names(cleanData) %in% GAM.predictors]]
GAM.data$SalePrice = cleanData$SalePrice

##Create Train and Test Data
sample <- sample.int(n = nrow(GAM.data), size = floor(.75*nrow(GAM.data)), replace = F)
train.GAM <- GAM.data[sample, ]
test.GAM  <- GAM.data[-sample, ]
```

```{r GAM Model, include=FALSE, echo=FALSE}
#GAM Model
GAM.model = gam(SalePrice ~ ns(LotFrontage,3) +
                            ns(LotArea,3) +
                            ns(YearBuilt, 3) +
                            ns(YearRemodAdd,3) +
                            ns(BsmtFinSF1, 3) +
                            ns(TotalBsmtSF, 3) +
                            ns(GrLivArea, 3) +  
                            ns(WoodDeckSF, 3) +
                            ns(OpenPorchSF, 3) +
                            OverallQual +
                            OverallCond +
                            ExterQual +
                            BsmtQual +
                            HeatingQC +
                            CentralAir +
                            BsmtFullBath +
                            KitchenAbvGr +
                            KitchenQual +
                            Functional +
                            FireplaceQu +
                            GarageFinish +
                            GarageCars +
                            GarageQual +
                            PavedDrive +
                            MSZoning.FV +
                            MSZoning.RM +
                            Alley.Pave +
                            LotConfig.CulDSac+
                            Neighborhood.BrDale+
                            Neighborhood.BrkSide+
                            Neighborhood.ClearCr+
                            Neighborhood.Crawfor+
                            Neighborhood.Edwards+
                            Neighborhood.MeadowV+
                            Neighborhood.Mitchel+
                            Neighborhood.NridgHt+
                            Neighborhood.OldTown+
                            Neighborhood.Somerst+
                            Neighborhood.StoneBr+
                            Neighborhood.Veenker+
                            Condition1.Artery+
                            Condition1.Norm+
                            Condition1.RRAe+
                            Exterior1st.MetalSd+
                            Exterior1st.HdBoard+
                            Exterior1st.BrkFace+
                            BldgType.Twnhs+
                            BldgType.1Fam+
                            MasVnrType.Stone+
                            Foundation.BrkTil+
                            Foundation.PConc+
                            BsmtExposure.Gd+
                            BsmtExposure.No+
                            BsmtFinType1.GLQ+
                            BsmtFinType1.Unf+
                            BsmtFinType2.ALQ+
                            BsmtFinType2.BLQ+
                            Electrical.FuseA+
                            Fence.GdWo+
                            Fence.None+
                            SaleType.COD+
                            SaleType.New+
                            SaleType.WD+
                            SaleCondition.Abnorml 
                            , data = train.GAM )

#Predict and get RMSE
preds = predict(GAM.model, newdata = test.GAM)
mean((exp(preds) - exp(test.GAM$SalePrice))^2)^(1/2)

GAM.model2 = lm(SalePrice ~
                            ns(LotArea,4) +
                            ns(YearBuilt, 4) +
                            ns(BsmtFinSF1, 2) +
                            ns(TotalBsmtSF, 4) +
                            ns(GrLivArea, 4) +  
                            ns(OpenPorchSF, 3) +
                            OverallQual +
                            OverallCond +
                            CentralAir +
                            Functional +
                            FireplaceQu +
                            GarageCars +
                            Neighborhood.Crawfor+
                            Neighborhood.BrkSide+
                            Neighborhood.StoneBr+
                            Exterior1st.BrkFace+
                            BsmtExposure.Gd+
                            SaleCondition.Abnorml,
                            data = train.GAM)
preds = predict(GAM.model2, newdata = test.GAM)
mean((exp(preds) - exp(test.GAM$SalePrice))^2)^(1/2)
```
```{r Plot Attempt, echo=FALSE, fig.align = "center", fig.height = 6, fig.width = 7}
fit.YearBuilt = lm(SalePrice ~ ns(YearBuilt, 3), data = GAM.data)
YearBuilt.lims = range(GAM.data$YearBuilt)
YearBuilt.grid = seq(from = YearBuilt.lims[1], to = YearBuilt.lims[2])
pred = predict(fit.YearBuilt, newdata = list(YearBuilt=YearBuilt.grid), se=T)
plot(GAM.data$YearBuilt, GAM.data$SalePrice, xlab = "Year Built", ylab = "log(SalePrice)", col = "azure4" )
lines(YearBuilt.grid, pred$fit, col ="blue", lwd=2)
lines(YearBuilt.grid, pred$fit + 2*pred$se, lty="dashed", col = "red")
lines(YearBuilt.grid, pred$fit - 2*pred$se, lty="dashed", col = "red")
```

For the benefit of interpretibility we decided to try to build a simpler GAM and see if we could narrow down the number of predictors and not loose a statistically significant amount of prediction accuracy. We ran a partial F-test to make sure our reduced model was sufficient (actual p-value = 2.2e-16). We were able to get this model down to 18 predictors. Below is a table of the Predictors and their coefficient estimates along with a confidence interval and their P-values.

```{r Predictors, echo = FALSE, fig.align="center"}
tab_model(GAM.model2)
```


#                                           Conclusion
The goals of this project were to be able to predict the sale price of a house with reasonable accuracy and find the most important predictors that effect the sale price of a house. We accomplished both of these goals. 

Using a Multiple Linear Regression model we were able to significantly improve prediction accuracy over a baseline Null Model that predicts the mean for every new observation. The baseline Null model accuracy had a RMSE of \$79,513.67. while our Multiple Linear Regression models RMSE was \$21,767.80. This tells us there is a strong relationship between Sale Price and the predictors in our Data. Once we knew this we attempted a bunch of different methods to improve prediction accuracy and try to narrow down the "strongest" predictors. Our Lasso and GAM models ended up being the our best models in terms of prediction accuracy. We were able to get our RMSE down to around \$20,000. Our Lasso model improved prediction accuracy because the reduction in variance outweighed the slight increase in bias. Our GAM model kind of did both. We fit it on a subset of the original 177 predictors but we also allowed for more flexible non-linear fits to some of the predictors and therefore reduced bias and increased variance. In the end the best accuracy we could acheive was a RMSE of \$20,000.


Some of the strongest predictors in predicting sale price can be seen in the table above along with how accurate their coefficient estimates are. A lot of the strongest predictors, and their relationship with sale price, is very intuitive. The bigger, newer, higher quality the house the higher the sale price. We saw this in "The Data" section when we checked the predictors correlation with sale price and again in every subset selection method.  We did notice that people in Ames Iowa seem to really like their basements and are willing to pay for them. Joking aside, that also is pretty intuitive. A big finished basement just means a bigger house and therefore an increase in sale price.


Overall we learned a little bit about Housing Markets and a lot about Statistical Modeling, R, and trying to make interesting plots in R.
