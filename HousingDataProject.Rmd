---
title: "Housing Data Project"
author: "Justin Sheldon, Jeremy Swiatek"
date: "October 24, 2018"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: united
    highlight: tango
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(boot)
library(car)
library(plyr)
library(dummies)
library(ggplot2)
library(gridExtra)
library(leaps)
library(ggrepel)
library(ggthemes)
library(glmnet)
library(pls)
```
#Abstract
The puprose of this project is to predict the "Sale Price" of a house using some or all of the 79 explanatory variables in our data set. We used (name the different models used once we are finished) to find the model with the best predictive power.(talk a little about our best model). In addition to predicting "Sale Price" we also isolated (list of significant variables) that had the highest impact of Sale Price. (Talk about each and how it effect Sale Price). In conclusion we now know a little bit more about what effects the sale price of a house and can make more informed decisions on the biggest purchase most people make in their lifetime.

#Introduction
Location, Location, Location, is that all you really need to know about housing to make an informed decision on the value of a house? Being an informed consumer is always a good idea but especially on the largest purchase you will probably make in your lifetime. The goal of this project is to be able to predict the sale price of a house with a reasonably good accuracy. Just as important, if not more so, we also want to gain a deeper understanding of the relationship between different factors and the sale price of a house.

Another goal of this project is to learn more about and get experience with the different statistical learning methods we have covered in class this semester. We plan to try out a lot of the methods we covered in class this semester and discuss the strengths and weaknesses of each method as it pertains to our data set. Therefore this project will be broken up into sections for each of the different methods. There is a table of contents to the right that you can click to jump to any section you want to look at. We will try to keep each section as self contained as possible.


#The Data
The dataset we will be using is the House Prices dataset from www.kaggle.com. The Response Variable is SalePrice and there are 79 explanatory variables. There are 1460 observations in total in the training Data. We did a lot of data cleaning before building any models. Below is everything we did catagorized into sections. After all the data cleaning we ended up with a training set with 1458 observations and 178 variables (after dummy variable creation). This is the training set we will be using for all of our models.

```{r echo = FALSE}
trainData = read.csv("train.csv", stringsAsFactors = FALSE)
testData = read.csv("test.csv", stringsAsFactors = FALSE)
```

###  Dealing with NA's
Upon further inspection of the data we noticed we had some variables that were causing us trouble. There are 19 variables with missing data. For a lot of the categorical variables "NA's" really meant "None" so we fixed those by converting NA level to a "None" level. There was also a couple of observations that had one missing value for a given variable and we just fixed that by predicting the mean/mode of that variable for that observation. Any other additional modification is listed below along with the reason why we choose that modification.

* **LotFrontage:** Linear Feet of street connected to property. This one was a tough decision. It's missing 259/1460 approximately 18% of the data for this variable but we didnt want to lose the variable entirely. We decided to fill in the NA's with the mean value of the variable.
* **GarageYrBlt:** Garage Year Built. We replaced the NA's with the Year the house was built as long as it actualy had a garage. We expect GarageYrBlt and YearBuilt to be highly correlated and eventually one will probably have to be removed from the model anyways.
* **MiscFeature:** Miscellaneous feature. This variable was essentially worthless. Almost all of the data was missing and the data that did exist was 'tennis court' and 'other'. We removed this variable.

```{r, Finding the NAs, include=  FALSE, comment = ""}
NAVariables <- which(colSums(is.na(trainData)) > 0)
sort(colSums(sapply(trainData[NAVariables], is.na)), decreasing = TRUE)
```
```{r, Fixing NA Variables, echo=FALSE, include=FALSE}
#Fixing Pool QC
trainData$PoolQC[is.na(trainData$PoolQC)] = 'None'
Qualities = c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)
trainData$PoolQC = as.integer(revalue(trainData$PoolQC, Qualities))

#Fixing Alley
trainData$Alley[is.na(trainData$Alley)] = 'None'
trainData$Alley = as.factor(trainData$Alley)

#Fixing Fence
trainData$Fence[is.na(trainData$Fence)] = 'None'
trainData$Fence = as.factor(trainData$Fence)

#Fixing Fireplace
trainData$FireplaceQu[is.na(trainData$FireplaceQu)] = 'None'
trainData$FireplaceQu = as.integer(revalue(trainData$FireplaceQu, Qualities))

#Fixing LotFrontage and other lot variables
for (i in 1:nrow(trainData)){
        if(is.na(trainData$LotFrontage[i])){
               trainData$LotFrontage[i] = as.integer(mean(trainData$LotFrontage, na.rm = TRUE))
        }
}
trainData$LotShape = as.integer(revalue(trainData$LotShape, c('IR3'=0, 'IR2'=1, 'IR1'=2, 'Reg'=3)))
trainData$LotConfig = as.factor(trainData$LotConfig)

#Fixing Garage variables
#Year Built
trainData$GarageYrBlt[is.na(trainData$GarageYrBlt)] = trainData$YearBuilt[is.na(trainData$GarageYrBlt)]
#Garage Type
trainData$GarageType[is.na(trainData$GarageType)] = 'None'
trainData$GarageType = as.factor(trainData$GarageType)
#Garage Finish
trainData$GarageFinish[is.na(trainData$GarageFinish)] = 'None'
FinishLevels = c('None'=0, 'Unf'=1, 'RFn'=2, 'Fin'=3)
trainData$GarageFinish<-as.integer(revalue(trainData$GarageFinish, FinishLevels))
#Garage Quality
trainData$GarageQual[is.na(trainData$GarageQual)] = 'None'
trainData$GarageQual = as.integer(revalue(trainData$GarageQual, Qualities))
#Garage Condition
trainData$GarageCond[is.na(trainData$GarageCond)] = 'None'
trainData$GarageCond = as.integer(revalue(trainData$GarageCond, Qualities))

#Fixing Basement Variables
#Additional NA in these two observations
trainData[!is.na(trainData$BsmtFinType1) & (is.na(trainData$BsmtCond)|is.na(trainData$BsmtQual)|is.na(trainData$BsmtExposure)|is.na(trainData$BsmtFinType2)), c('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')]
#Predict Most common occurence to fix these two observations
trainData$BsmtFinType2[333] = "Unf"
trainData$BsmtExposure[949] = "No"
#Basement Quality
trainData$BsmtQual[is.na(trainData$BsmtQual)] = 'None'
trainData$BsmtQual = as.integer(revalue(trainData$BsmtQual, Qualities))
#Basement Condition
trainData$BsmtCond[is.na(trainData$BsmtCond)] = 'None'
trainData$BsmtCond = as.integer(revalue(trainData$BsmtCond, Qualities))
#Basement Exposure
trainData$BsmtExposure[is.na(trainData$BsmtExposure)] = 'None'
trainData$BsmtExposure = as.factor(trainData$BsmtExposure)
#Basement Finish Type 1
trainData$BsmtFinType1[is.na(trainData$BsmtFinType1)] = 'None'
trainData$BsmtFinType1 = as.factor(trainData$BsmtFinType1)
#Basement Finish Type 2
trainData$BsmtFinType2[is.na(trainData$BsmtFinType2)] = 'None'
trainData$BsmtFinType2 = as.factor(trainData$BsmtFinType2)

#Fixing Masonry Variables
#Masonary Veneer Type
trainData$MasVnrType[is.na(trainData$MasVnrType)] = 'None'
trainData$MasVnrType = as.factor(trainData$MasVnrType)
#Masonary Veneer Area
trainData$MasVnrArea[is.na(trainData$MasVnrArea)] = 0

#Fixing Electrical
trainData$Electrical[is.na(trainData$Electrical)] = "SBrkr"
trainData$Electrical = as.factor(trainData$Electrical)


#Fixing MiscFeature (junk just remove)
trainData = subset(trainData, select = -MiscFeature)
```
```{r, Fixing Character Variables, echo=FALSE,include=FALSE}
#Turning Char variables into factor variables
trainData$MSZoning = as.factor(trainData$MSZoning)
trainData$Street = as.factor(trainData$Street)
trainData$LandContour = as.factor(trainData$LandContour)
trainData$Neighborhood = as.factor(trainData$Neighborhood)
trainData$Condition1 = as.factor(trainData$Condition1)
trainData$BldgType = as.factor(trainData$BldgType)
trainData$HouseStyle = as.factor(trainData$HouseStyle)
trainData$RoofStyle = as.factor(trainData$RoofStyle)
trainData$Exterior1st = as.factor(trainData$Exterior1st)
trainData$Exterior2nd = as.factor(trainData$Exterior2nd)
trainData$Foundation = as.factor(trainData$Foundation)
trainData$Heating = as.factor(trainData$Heating)
trainData$SaleType = as.factor(trainData$SaleType)
trainData$SaleCondition = as.factor(trainData$SaleCondition)


#Turning Char Variables into Ordinal Numeric
trainData$LandSlope = as.integer(revalue(trainData$LandSlope, c('Sev'=0, 'Mod'=1, 'Gtl'=2)))
trainData$ExterQual = as.integer(revalue(trainData$ExterQual, Qualities))
trainData$ExterCond = as.integer(revalue(trainData$ExterCond, Qualities))
trainData$HeatingQC = as.integer(revalue(trainData$HeatingQC, Qualities))
trainData$CentralAir = as.integer(revalue(trainData$CentralAir, c('N'=0, 'Y'=1)))
trainData$KitchenQual = as.integer(revalue(trainData$KitchenQual, Qualities))
trainData$Functional = as.integer(revalue(trainData$Functional, c('Sal'=0, 'Sev'=1, 'Maj2'=2, 'Maj1'=3, 'Mod'=4, 'Min2'=5, 'Min1'=6, 'Typ'=7)))
trainData$PavedDrive = as.integer(revalue(trainData$PavedDrive, c('N'=0, 'P'=1, 'Y'=2)))

#Not a part of the data just observation #
trainData = subset(trainData, select = -Id)

#Make sure Empty
charVariables = names(trainData[,sapply(trainData, is.character)])
charVariables
```

###  Dealing with Multicollinearity
We decided to just deal with multicollinearity issues before building any models. We checked the correlation matrix for all numeric variables and decided to remove any pair of variables with a correlation of 0.75 or higher. Of the two variables we kept Whichever variable had a higher correlation with SalePrice. Below is a list of all of these pairs and which variables we kept.

* **Garage Yr Blt vs. Year Built:** correlation = 0.845. We kept Year Built
* **Garage Area vs. Garage Cars:** correlation = 0.882. We kept Garage Cars
* **Total Basement SqFt vs. 1st Floor SqFt** correlation = 0.819. We kept Total Basement SqFt
* **Garage Condition vs. Garage Quality** correlation = 0.959. We kept Garage Quality
* **Total Rooms Above Ground vs. Ground Living Area** correlation = 0.825. We kept Ground Living Area

```{r Multicollinearity, include=FALSE, echo=FALSE}
#Finding the Correlated Variables
    # numericVars <- which(sapply(trainData, is.numeric))
    # all_numVar <- trainData[, numericVars]
    # #Garage Year Build vs YearBuilt
    # print(cor(all_numVar, trainData$GarageYrBlt) > 0.75)
    # print(cor(trainData$GarageYrBlt, trainData$SalePrice))
    # print(cor(trainData$YearBuilt, trainData$SalePrice))
    # #Garage Garage Cars vs GarageArea
    # print(cor(trainData$GarageArea, trainData$GarageCars))
    # print(cor(trainData$GarageArea, trainData$SalePrice))
    # print(cor(trainData$GarageCars, trainData$SalePrice))
    # #TotalBsmtSF vs. X1stFlrSF
    # print(cor(trainData$TotalBsmtSF, all_numVar) > 0.75)
    # print(cor(trainData$TotalBsmtSF, trainData$X1stFlrSF))
    # print(cor(trainData$TotalBsmtSF, trainData$SalePrice))
    # print(cor(trainData$X1stFlrSF, trainData$SalePrice))
    # #GarageCond vs. Garage Quality
    # print(cor(trainData$GarageCond, all_numVar) > 0.75)
    # print(cor(trainData$GarageCond, trainData$GarageQual))
    # print(cor(trainData$GarageCond, trainData$SalePrice))
    # print(cor(trainData$GarageQual, trainData$SalePrice))
    # #Total Rooms Above Ground vs. Ground Living Area
    # print(cor(trainData$TotRmsAbvGrd, all_numVar) > 0.75)
    # print(cor(trainData$TotRmsAbvGrd, trainData$GrLivArea))
    # print(cor(trainData$TotRmsAbvGrd, trainData$SalePrice))
    # print(cor(trainData$GrLivArea, trainData$SalePrice))
    
#Remove the Variables
trainData = subset(trainData, select = -GarageYrBlt)
trainData = subset(trainData, select = -GarageArea)
trainData = subset(trainData, select = -X1stFlrSF)
trainData = subset(trainData, select = -GarageCond)
trainData = subset(trainData, select = -TotRmsAbvGrd)

#Check Removed
#dim(trainData)
```

###  Dealing with Sparse Data
Some of our factor variables had levels with a count of less then 5. This was causing problems with using cross validation. Below is a list of all the variables that had this issue and how we dealth with them.

* **Utilities:** Type of utilities available. 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Condition2:** Proximity to various conditions. 8 levels with 99% of the data belonging to one level. We removed this variable.
* **RoofMatl1:** Roof Material. 8 levels with more than 98% of the data belonging to one level. We removed this variable.
* **Street:** 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Heating:** 6 levels with 98% of the data belonging to one level. We removed this variable.

We kept the rest of the variables that had sparse levels in the model because they had other levels that were not sparse and could be important in future models. We converted all the factor variables into dummy variables and then just removed the dummy variables with levels that had less than 10 observations. This ended up being 32 out of 162 levels.

```{r Looking at Catagorical Variables, echo=FALSE, include=FALSE}
#Find Categorical Variables
numericVars = which(sapply(trainData, is.numeric))
all_factorVars = trainData[,-numericVars]
print(names(all_factorVars))
```
```{r Remove Worthless Variables, include=FALSE,echo=FALSE}
#Too Sparse < 2% of data not in same category
trainData = subset(trainData, select = -Utilities)
trainData = subset(trainData, select = -Condition2)
trainData = subset(trainData, select = -RoofMatl)
trainData = subset(trainData, select = -Street)
trainData = subset(trainData, select = -Heating)
```
```{r Dummy Variables and new Train DF, include=FALSE, echo=FALSE}
#Find all the factor variables
factorVars = which(sapply(trainData, is.factor))
factorVars = names(factorVars)
#Create seperate data frame to convert factor variables to dummy varaibles
DFfactors = trainData[, factorVars]
DFdummies = dummy.data.frame(DFfactors, sep = ".")
#Find Sparse dummy variables
sparseLevels <- which(colSums(DFdummies[1:nrow(trainData[!is.na(trainData$SalePrice),]),])<10)
#Remove Sparse dummy variables
DFdummies = DFdummies[,-sparseLevels]
#combine all the non factor variables in original data frame with our new dummy variables
DFnumeric = trainData[,!(names(trainData) %in% factorVars)]
trainDF.dummies = cbind(DFnumeric, DFdummies)
```



###  Dealing with Skewed Response Variable
The response variable SalePrice was heavily right skewed. We fixed this with a log transformation. Below is a couple before and after graphs showing the results. We will be using the log(SalePrice) as our predictor variable in our models.

```{r Create Log Sale Price Data Frame, include=FALSE, echo=FALSE}
trainDF.log = trainDF.dummies
trainDF.log$SalePrice = log(trainDF.log$SalePrice)
```
```{r Plot of SalePrice vs. Normal Distribution , echo=FALSE , fig.align = "center", fig.height = 5, fig.width = 7}
p1 = ggplot(trainDF.dummies, aes(x=SalePrice)) +
        geom_histogram(aes(y = ..density..), fill="blue", binwidth = 10000) +
        stat_function(fun = dnorm, args = list(mean(trainDF.dummies$SalePrice),sd(trainDF.dummies$SalePrice)), colour = "red", lwd = 1) +
        scale_x_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000")) +
        labs(title = "SalePrice vs. Normal Distribution")

p2 = ggplot(trainDF.log, aes(x=SalePrice)) +
        geom_histogram(aes(y = ..density..), fill="blue", binwidth = .15) +
        stat_function(fun = dnorm, args = list(mean(trainDF.log$SalePrice),sd(trainDF.log$SalePrice)), colour = "red", lwd = 1) +
        scale_x_continuous(breaks= seq(10, 14, by=.5)) +
        labs(title = "Log SalePrice vs. Normal Distribution")

grid.arrange(p1, p2, nrow = 1, ncol = 2)
```
```{r QQ Plots, echo=FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
#QQ PLots
par(mfrow = c(1,2))
qqnorm(trainDF.dummies$SalePrice, main = "Sale Price QQ Plot")
qqline(trainDF.dummies$SalePrice)
qqnorm(trainDF.log$SalePrice, main = "Log Sale Price QQ Plot")
qqline(trainDF.log$SalePrice)
```

### Dealing with Outliers
The only outliers we found were observations 524 and 1299. We deemed these outliers significant enough to remove because they were huge houses with a overall quality rating of 10 (the highest rating) and yet they sold for a very low price. By looking at the size and quality rating of the house if appears they may have missed a 0 on the end of the price. Size of the house (GrLiveArea) and Overall Quality are the two highest positive correlated variables with SalePrice which lead us to believe these SalePrices were a mistake and we removed these observations from the data set. We left the graphs below in terms of the actual Sale Price and not log of the Sale Price so it would be easier to interpret

```{r Finding Outliers, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6, comment=""}
ggplot(data=trainDF.dummies[!is.na(trainDF.dummies$SalePrice),], aes(x=GrLivArea, y=SalePrice))+
        geom_point(col='blue') + geom_smooth(method = "lm", se=FALSE, color="red", aes(group=1)) +
        scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000") ) +
        geom_text_repel(aes(label = ifelse(trainDF.dummies$GrLivArea[!is.na(trainDF.dummies$SalePrice)]>4500, rownames(trainDF.dummies), '')))
trainDF.dummies[c(524, 1299), c('SalePrice', 'GrLivArea', 'OverallQual')]
```

```{r final Data to be used for models, echo=FALSE, include=FALSE}
#Use cleanData for all future models
cleanData = trainDF.log
cleanData = cleanData[-c(524,1299),]
```


#Multiple Linear Regression
The main purpose of building a Multiple Linear Regression model is to use it as a benchmark that we compare all future models to. Below is a brief summary of our Multiple Linear Regression model that includes all variables in the model fit. 

* 10 Fold Cross Valiation Error Rate: (waiting for clean data)
* Most Significant Parameters: 
* Non-Significant Parameters: 

```{r, Base Model, echo=FALSE, include=FALSE}
baseModel = glm(SalePrice ~ ., data = trainDF.dummies)
summary(baseModel)
```

```{r, Cross Validation, comment="", include=FALSE, echo=FALSE}
set.seed(1)
cvTrainData = trainDF.dummies
cv.error10 = cv.glm(cvTrainData, baseModel, K=10)$delta[1]
```

##  Potential Problems

####     Residual Plots
From the Residual plots it appears as if a linear model may not be a good representation of the data. We plan to try out some Model selection and regularization methods and see if this pattern still exists. If so we will use non-linear methods.

```{r, Model Residuals, echo=FALSE, include=FALSE}
par(mfrow= c(2,2))
plot(baseModel)
```

####     Model Outliers
After looking at the residual plot we noticed there are 10 outliers that may be influencing the model fit. The points influncing the data are listed below. We will keep an eye on these observations in future models and see if they continue to be a problem. We will deal with them later if we need to.
```{r, Model Outliers, echo=FALSE, comment=""}
outlierTest(baseModel)
```



#Model Selection and Regularization

##Subset Selection
The goal of subset Selection is to try and find the smallest subset of the predictors that are statistically significant and still lead to a good model fit. The advantage to an approach like this over Multiple Linear Regression is that using a subset of the original predictors can lead to a significant reduction in variance while only slightly increasing the bias. Also it simplifies model inference and kind of allows us to see which variables are "most important". Subset Selection still uses Least Squares fits in the form of  $Y = \beta_0 +\sum_{i=1}^p \beta_i X_i + \epsilon$.  Our hope is that a subset selection model will out perform the full model we used in Multiple Linear Regression Section. Below we perform both Forward stepwise Selection and Backwards Stepwise Selection and give more details about the methods used and the results.

```{r Stuff We Need for Subset Selection, include=FALSE, echo=FALSE}
subSetData = cleanData
```

####Forward Stepwise Selection
To perform forward stepwise selection we fit a model that iteratively steps forward adding the next most significant predictor at each iteration. We then used Cross Validation on each model from 1-177 predictors to find the best model size for our data. We also looked at the CP, BIC and Adjusted R^2 scores to decide on the best model size. Below is graphs of all 4 methods we used in picking the best model size.

```{r, Forward Stepwise Selection, include=FALSE, echo=FALSE}
#CP, BIC, AdjR^2
regfit.fwd = regsubsets(SalePrice ~., data = subSetData, nvmax = 160, method = "forward")
regfit.fwd.summary = summary(regfit.fwd)

#validation set 
set.seed(88)
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train <- subSetData[sample, ]
test  <- subSetData[-sample, ]

regfit.valid = regsubsets(SalePrice ~., data = train, nvmax = 160, method = "forward")
test.matrix = model.matrix(SalePrice ~., data = test)
validation.errors = rep(NA, 160)
for (i in 1:160){
    coefi = coef(regfit.valid, id=i)
    pred = test.matrix[,names(coefi)]%*%coefi
    validation.errors[i] = mean((test$SalePrice-pred)^2)
}

#Get Test RMSE on Sale Price
coefi2 = coef(regfit.valid, id=50)
pred2 = test.matrix[,names(coefi2)]%*%coefi2
mean(( exp(test$SalePrice) - exp(pred2) )^2)^(1/2)
```

```{r plots to include, echo=FALSE}
#set xlab abd ylab and points
par(mfrow = c(2,2))
plot(regfit.fwd.summary$adjr2, type = "l", xlab = "Number of Variables", ylab = "Adjusted R Squared")
points(which.max(regfit.fwd.summary$adjr2), regfit.fwd.summary$adjr2[which.max(regfit.fwd.summary$adjr2)], col="red", cex=2,pch=20)
plot(regfit.fwd.summary$bic, type = "l", ylab = "BIC", xlab = "Number of Variables")
points(which.min(regfit.fwd.summary$bic), regfit.fwd.summary$bic[which.min(regfit.fwd.summary$bic)], col="red", cex=2,pch=20)
plot(regfit.fwd.summary$cp, type = "l", ylab = "CP", xlab = "Number of Variables")
points(which.min(regfit.fwd.summary$cp), regfit.fwd.summary$cp[which.min(regfit.fwd.summary$cp)], col="red", cex=2,pch=20)
plot(validation.errors, type = "l", ylab = "Validation MSE", xlab = "Number of Variables")
points(which.min(validation.errors), validation.errors[which.min(validation.errors)], col="red",cex=2,pch=20)
```

Forward stepwise Selection revealed that the best model using this method is somewhere around 40-50 variables in the model. Remember this is not necessarly the best model because forward stepwise can get stuck in a suboptimal path. That being said it appears like around 50 variables will turn out to be a good model. Below is a report of the model with 50 variables vs the full Least Squares Model.

* **Best Model Size:** 50
* **Test RMSE FWD Stepwise Selction:** $33,193.09
* **Test RMSE Least Squares:** $21,543.60
* **Test RMSE Null Model:** $79,513.67
*We changed the Sale Price back into dollars instead of log(SalePrice) because it was easier to interpret. Also note this is the Root Mean Squared Error which can be thought of as how close our predictions are on average.

Surprisingly this model did very poorly. As we kept increasing the number of predictors the validation set RMSE kept decreasing. It appears our dataset does not have a problem with high variance and the reduction in variance we gained from a smaller model did not outweigh the increase in bias we incurred from using a smaller model. 

The most interesting thing we found from doing forward stepwise was just determining which variables were selected first and therefore are potentially important variables in predicting sale price.

**First 10 Variables picked in model**

* **OverallQual:** Rates the overall material and finish of the house.
* **BsmtFullBath:** Number of full bathrooms in the basement.
* **YearBuilt:** Original Construction Date.
* **OverallCond:** Rates the overall condition of the house.
* **GarageQual:** Rates the Quality of the Garage.
* **HeatingQC:** Heating Quality and Condition Rating.
* **Alley.none:** Dummy Variable representing no alleyway access to the house.
* **BsmtHalfBath:** Number of half bathrooms in the basement.
* **GarageFinish:** Rating of Interior finish of the garage.
* **MSZoning.FV:** Dummy Variable for Zoning Code for "Floating Village Residential"


####Backwards Stepwise Selection
The goal, methods and equation for backwards stepwise selection is the same as that of forward stepwise selection so we will not restate them all here.

```{r, Backwards Stepwise Selection, include=FALSE, echo=FALSE}
#CP, BIC, AdjR^2
regfit.back = regsubsets(SalePrice ~., data = subSetData, nvmax = 160, method = "backward")
regfit.back.summary = summary(regfit.back)

#validation set 
set.seed(88)
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train <- subSetData[sample, ]
test  <- subSetData[-sample, ]

regfit.valid = regsubsets(SalePrice ~., data = train, nvmax = 160, method = "backward")
test.matrix = model.matrix(SalePrice ~., data = test)
validation.errors = rep(NA, 160)
for (i in 1:160){
    coefi = coef(regfit.valid, id=i)
    pred = test.matrix[,names(coefi)]%*%coefi
    validation.errors[i] = mean((test$SalePrice-pred)^2)
}
coefi2 = coef(regfit.valid, id=50)
pred2 = test.matrix[,names(coefi2)]%*%coefi2
mean(( exp(test$SalePrice) - exp(pred2) )^2)^(1/2)

```
```{r plots for backstep, echo=FALSE}
par(mfrow = c(2,2))
plot(regfit.back.summary$adjr2, type = "l", xlab = "Number of Variables", ylab = "Adjusted R Squared")
points(which.max(regfit.back.summary$adjr2), regfit.back.summary$adjr2[which.max(regfit.back.summary$adjr2)], col="red", cex=2,pch=20)
plot(regfit.back.summary$bic, type = "l", ylab = "BIC", xlab = "Number of Variables")
points(which.min(regfit.back.summary$bic), regfit.back.summary$bic[which.min(regfit.back.summary$bic)], col="red", cex=2,pch=20)
plot(regfit.back.summary$cp, type = "l", ylab = "CP", xlab = "Number of Variables")
points(which.min(regfit.back.summary$cp), regfit.back.summary$cp[which.min(regfit.back.summary$cp)], col="red", cex=2,pch=20)
plot(validation.errors, type = "l", ylab = "Validation MSE", xlab = "Number of Variables")
points(which.min(validation.errors), validation.errors[which.min(validation.errors)], col="red",cex=2,pch=20)
```

Backwards Stepwise Selection ends up with pretty much the same results as forward stepwise selection. Somewhere between 40-50 variables seems to be when the graphs begin to flatten out. We decided to build a model with 50 predictors and use a validation set to estimate the RMSE. Below are the results.

* **Best Model Size:** 50
* **Test RMSE FWD Stepwise Selction:** $27,823.40
* **Test RMSE Least Squares:** $21,543.60
* **Test RMSE Null Model:** $79,513.67

This model outperforms Forward Stepwise Selection but still does not outperform the Full Least Squares model. Again it appears like the reduction in varaiance we gain from fitting a simpler model do not outweigh the increase in bias.

**Last 10 Variables left in model**

* **OverallQual:** Rates the overall material and finish of the house.
* **BsmtFullBath:** Number of full bathrooms in the basement.
* **YearBuilt:** Original Construction Date.
* **BsmtFinSF1:** How many square feet of the basement that is finished.
* **OverallCond:** Rates the overall condition of the house.
* **GarageQual:** Rates the Quality of the Garage.
* **BsmtUnfSF:** Unfinished square feet of basement area.
* **LotArea:** Lot Size in Square Feet
* **BsmtFinSF2:** Rating of basment finished area.
* **Neighborhood.MeadowV:** Neighboorhood location Meadow Village

I think it is interesting to note that both subset selection methods find that having a finished basement with bathrooms seems to be important factor in predicting SalePrice (after accounting for Overall Quality). Also we see that a neighborhood appeared in the top 10 here. The coefficient for this neighborhood is negative so we dug a little deeper to see this neighboorhoods relationship with sale price in comparison to other neighborhoods. As you can see from the plot below Meadown Village has the lowest median sale price of all neighborhoods therefore this coefficient makes sense. Also it makes sense that Meadow Village would be a significant factor in predicting sale price because it appears to be one of the worst neighborhoods in terms of sales price in Ames Iowa.

```{r a look at Neighborhoods, echo=FALSE}
neighbor.plot = ggplot(trainData, aes(Neighborhood, SalePrice)) +
                geom_boxplot(aes(fill = factor(Neighborhood)))  +
                theme(axis.text.x = element_text(angle = 65, vjust = 0.6)) +
                labs( x = "Neighborhoods", y = "Sale Price") +
                theme(legend.position="none") +
                scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000") ) +
                geom_hline(yintercept = 88000, linetype="dashed", color = "red")
neighbor.plot
```

##Shrinkage Methods
####Ridge Regression
The goal of Ridge Regression is to find a set of coefficients that minimize  $RSS +\lambda\sum_{j=1}^p \beta^2_j$.  The summation terms works as a shrinkage penalty that incentivises the model to srink the coefficients towards 0. The hope is that by doing this we will significantly reduce model variance while only slightly increasing model bias and therefore be able to outperform the full Least Squares model. 

The equation aboves shrinkage penalty depends on the value of lambda you choose. In order to find the best value for lambda we performed cross validation on a lot of different lambda values. Below is a graph of the coefficients for different levels of lambda along with the best lambda we found using Cross Validation. (note: the function we use to fit the models automatically performs variable regularization)

```{r, Ridge Regression, include=FALSE, echo=FALSE}
#Setup
set.seed(1)
x = model.matrix(SalePrice ~., cleanData)[,-1]
y = cleanData$SalePrice
train = sample(1:nrow(x), nrow(x)/1.5)
test = (-train)
y.test= y[test]
ridge.mod = glmnet(x[train,],y[train],alpha = 0)

#Find best lamda value
cv.out = cv.glmnet(x[train,],y[train], alpha = 0)
bestLambda = cv.out$lambda.min
bestLambda

#test RMSE for best lambda
ridge.pred = predict(ridge.mod, s = bestLambda, newx = x[test,])
(mean((exp(ridge.pred)-exp(y.test))^2))^(1/2)
#test RMSE for least squares model
ls.pred = predict(ridge.mod, s = 0.00001, newx = x[test,])
(mean((exp(ls.pred)-exp(y.test))^2))^(1/2)
#baseline null model predictions
mean(( mean(exp(y[train])) -exp(y.test) )^2)^(1/2)
```
```{r, Ridge Regression plot, echo=FALSE}
plot(ridge.mod, xvar="lambda")
legend(1.75, -.125, legend=c("Best Lambda", "MSZoning.C (all)", "Neighborhood.MeadowV", "Exterior1st.BrkFace", "Neighborhood.Crawfor", "Neighborhood.StoneBr"),
       col=c("red","magenta", "black", "red", "green", 69), lty=c(2,1,1,1,1,1), cex=1)
abline(v = log(bestLambda), col = "red", lwd = 2, lty = 2)
```

The best lambda we found is close to 0 and therefore it looks like this model will be close to the full Least Squares model. This is not a huge surprise because we noticed in our subset selection models that a lot of predictors were significant. Also our sample size is a lot larger then the number of predictors 1458 >> 177 so reducing variance may not be a huge concern for us and the increase in bias may be a bigger problem. Below is a comparison in the model accuracies.

* **Best Lambda:** 0.044
* **Test RMSE Ridge:** $21,646.23
* **Test RMSE Least Squares:** $21,543.60
* **Test RMSE Null Model:** $79,513.67

The Ridge Regression model and the least squares model perform pretty much identically on a validation set at lamda = 0.044. They both significantly beat the Null Model that only uses the mean of Sale Price to predict Sale Price but essentially they are pretty much the same model.

####Lasso Regression
The goal of Lasso Regression is similar to Ridge Regression but in Lasso Regression we also perform subset selection. This has the benefit of potentially reducing variance and therefore improving prediction accuracy while also improving interpretibilty. We hope this model will outperform the full Least Squares while also being a lot more interpretible than Ridge Regression. The equation we use to do this is similar to Ridge Regression but now we minimize $RSS +\lambda\sum_{j=1}^p\lvert \beta_j \rvert$. Again we need to find the best lambda and we use the same methods as before.


```{r, Lasso, include=FALSE, echo=FALSE}
#Use train and test from before 

#Baseline Lasso Model
lasso.mod = glmnet(x[train,], y[train], alpha = 1)

#Find best Lambda
cv.lasso.out = cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.lasso.out)
bestLassoLambda = cv.lasso.out$lambda.min
bestLassoLambda

#Lasso Predictions accuracy
lasso.pred = predict(lasso.mod, s = bestLassoLambda, newx = x[test,])
(mean((exp(lasso.pred)-exp(y.test))^2))^(1/2)

#Number of Non Zero coefficients at best lambda
lasso.coef = predict(lasso.mod, type = "coefficients", s=bestLassoLambda)
sum(lasso.coef!=0)
```

```{r, Lasso Regression plot, echo=FALSE}
plot(lasso.mod, xvar="lambda")
legend(-4, -.275, legend=c("Best Lambda", "MSZoning.C (all)", "Neighborhood.MeadowV", "Neighborhood.Crawfor"), col=c("red", 69, "black", "blue" ), lty=c(2,1,1,1), pt.cex=1,  cex = 0.75)
abline(v = log(bestLassoLambda), col = "red", lwd = 2, lty = 2)
```

The Lasso model with the lowest cross validation error rate is a model with 71 predictors. This is close to we saw in forward and backward stepwise selection. 

* **Best Lambda:** 0.0039
* **Test RMSE Lasso:** $20,248.17
* **Test RMSE Least Squares:** $21,543.60
* **Test RMSE Null Model:** $79,513.67
* **Count of Non-Zero Coefficients:** 71

Although this helps with inference and interpretibility because of having fewer variables it doesn't help all that much with prediction. Again the Test RMSE is approximately the same between the Lasso model and the full Least Squares model. Currently this is the best model for both prediction and interpretibility we have.

##Dimension Reduction Methods
####Principle Components Regression
The goals of principle components regression is to find principle components of the form  Z = $\sum_{i=1}^n \phi_jX_j$  such that a few principle components can be used to predict the response. The hope is that this will lead to a substantial reduction in variance while only causing a slight increase in bias. Note that each principle component is a linear cobination of all the predictors. We want this method to outperform the Least Squares. This has an advantage over Least Squares in that it will have a lower variance but it will have a higher bias because we are assuming a simpler model. 

We fit a Principle Components Regression model (with scale = TRUE to regularize the data) and used cross validation to determine the optimal number of principal components. Below is a plot of this model fit. After finding the optimal number of components we fit a new model only on a training data set and then used a validation set to assess the accuracy of a PCR model with 30 components. Below is a description of the results.

* **Optimal Number of Principle Components:** 30
* **Test RMSE PCR:** $25,735.92
* **Test RMSE Least Squares:** $21,543.60
* **Test RMSE Null Model:** $79,513.67

Unfortunately again we see that the decrease in variance from a dimension reduction method does not help us. Our model does not appear to have high variance issues. This is reasonable because our number of observations is significantly higher than the number of predictors. Hopefully we will have better results with methods that reduce bias.

```{r, PCR, include=FALSE, echo=FALSE}
set.seed(1)
#Find best number of principle components
pcr.fit = pcr(SalePrice ~., data = cleanData, scale = TRUE, validation = "CV")
#summary(pcr.fit)

#Validation Set Results
pcr.fittrain = pcr(SalePrice ~., data = cleanData, subset = train, scale = TRUE, validation = "CV")
pcr.pred = predict(pcr.fittrain, x[test,], ncomp = 170)
mean((exp(pcr.pred) - exp(y.test))^2)^(1/2)
```

```{r, PCR Plots, echo=FALSE}
validationplot(pcr.fit, val.type = "MSEP")
```





