---
title: "Housing Data Project"
author: "Justin Sheldon, Jeremy Swiatek"
date: "October 24, 2018"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: united
    highlight: tango
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(boot)
library(car)
library(plyr)
library(dummies)
library(ggplot2)
library(gridExtra)
library(leaps)
library(ggrepel)
library(ggthemes)
library(glmnet)
```
#Abstract
The puprose of this project is to predict the "Sale Price" of a house using some or all of the 79 explanatory variables in our data set. We used (name the different models used once we are finished) to find the model with the best predictive power.(talk a little about our best model). In addition to predicting "Sale Price" we also isolated (list of significant variables) that had the highest impact of Sale Price. (Talk about each and how it effect Sale Price). In conclusion we now know a little bit more about what effects the sale price of a house and can make more informed decisions on the biggest purchase most people make in their lifetime.

#Introduction
Location, Location, Location, is that all you really need to know about housing to make an informed decision on the value of a house? Being an informed consumer is always a good idea but especially on the largest purchase you will probably make in your lifetime. The goal of this project is to be able to predict the sale price of a house with a reasonably good accuracy. Just as important, if not more so, we also want to gain a deeper understanding of the relationship between different factors and the sale price of a house.

Another goal of this project is to learn more about and get experience with the different statistical learning methods we have covered in class this semester. We plan to try out a lot of the methods we covered in class this semester and discuss the strengths and weaknesses of each method as it pertains to our data set. Therefore this project will be broken up into sections for each of the different methods. There is a table of contents to the right that you can click to jump to any section you want to look at. We will try to keep each section as self contained as possible.


#The Data
The dataset we will be using is the House Prices dataset from www.kaggle.com. The Response Variable is SalePrice and there are 79 explanatory variables. There are 1460 observations in total in the training Data. We did a lot of data cleaning before building any models. Below is everything we did catagorized into sections. After all the data cleaning we ended up with a training set with 1458 observations and 178 variables (after dummy variable creation). This is the training set we will be using for all of our models.

```{r echo = FALSE}
trainData = read.csv("train.csv", stringsAsFactors = FALSE)
testData = read.csv("test.csv", stringsAsFactors = FALSE)
```

###  Dealing with NA's
Upon further inspection of the data we noticed we had some variables that were causing us trouble. There are 19 variables with missing data. For a lot of the categorical variables "NA's" really meant "None" so we fixed those by converting NA level to a "None" level. There was also a couple of observations that had one missing value for a given variable and we just fixed that by predicting the mean/mode of that variable for that observation. Any other additional modification is listed below along with the reason why we choose that modification.

* **LotFrontage:** Linear Feet of street connected to property. This one was a tough decision. It's missing 259/1460 approximately 18% of the data for this variable but we didnt want to lose the variable entirely. We decided to fill in the NA's with the mean value of the variable.
* **GarageYrBlt:** Garage Year Built. We replaced the NA's with the Year the house was built as long as it actualy had a garage. We expect GarageYrBlt and YearBuilt to be highly correlated and eventually one will probably have to be removed from the model anyways.
* **MiscFeature:** Miscellaneous feature. This variable was essentially worthless. Almost all of the data was missing and the data that did exist was 'tennis court' and 'other'. We removed this variable.

```{r, Finding the NAs, include=  FALSE, comment = ""}
NAVariables <- which(colSums(is.na(trainData)) > 0)
sort(colSums(sapply(trainData[NAVariables], is.na)), decreasing = TRUE)
```
```{r, Fixing NA Variables, echo=FALSE, include=FALSE}
#Fixing Pool QC
trainData$PoolQC[is.na(trainData$PoolQC)] = 'None'
Qualities = c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)
trainData$PoolQC = as.integer(revalue(trainData$PoolQC, Qualities))

#Fixing Alley
trainData$Alley[is.na(trainData$Alley)] = 'None'
trainData$Alley = as.factor(trainData$Alley)

#Fixing Fence
trainData$Fence[is.na(trainData$Fence)] = 'None'
trainData$Fence = as.factor(trainData$Fence)

#Fixing Fireplace
trainData$FireplaceQu[is.na(trainData$FireplaceQu)] = 'None'
trainData$FireplaceQu = as.integer(revalue(trainData$FireplaceQu, Qualities))

#Fixing LotFrontage and other lot variables
for (i in 1:nrow(trainData)){
        if(is.na(trainData$LotFrontage[i])){
               trainData$LotFrontage[i] = as.integer(mean(trainData$LotFrontage, na.rm = TRUE))
        }
}
trainData$LotShape = as.integer(revalue(trainData$LotShape, c('IR3'=0, 'IR2'=1, 'IR1'=2, 'Reg'=3)))
trainData$LotConfig = as.factor(trainData$LotConfig)

#Fixing Garage variables
#Year Built
trainData$GarageYrBlt[is.na(trainData$GarageYrBlt)] = trainData$YearBuilt[is.na(trainData$GarageYrBlt)]
#Garage Type
trainData$GarageType[is.na(trainData$GarageType)] = 'None'
trainData$GarageType = as.factor(trainData$GarageType)
#Garage Finish
trainData$GarageFinish[is.na(trainData$GarageFinish)] = 'None'
FinishLevels = c('None'=0, 'Unf'=1, 'RFn'=2, 'Fin'=3)
trainData$GarageFinish<-as.integer(revalue(trainData$GarageFinish, FinishLevels))
#Garage Quality
trainData$GarageQual[is.na(trainData$GarageQual)] = 'None'
trainData$GarageQual = as.integer(revalue(trainData$GarageQual, Qualities))
#Garage Condition
trainData$GarageCond[is.na(trainData$GarageCond)] = 'None'
trainData$GarageCond = as.integer(revalue(trainData$GarageCond, Qualities))

#Fixing Basement Variables
#Additional NA in these two observations
trainData[!is.na(trainData$BsmtFinType1) & (is.na(trainData$BsmtCond)|is.na(trainData$BsmtQual)|is.na(trainData$BsmtExposure)|is.na(trainData$BsmtFinType2)), c('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')]
#Predict Most common occurence to fix these two observations
trainData$BsmtFinType2[333] = "Unf"
trainData$BsmtExposure[949] = "No"
#Basement Quality
trainData$BsmtQual[is.na(trainData$BsmtQual)] = 'None'
trainData$BsmtQual = as.integer(revalue(trainData$BsmtQual, Qualities))
#Basement Condition
trainData$BsmtCond[is.na(trainData$BsmtCond)] = 'None'
trainData$BsmtCond = as.integer(revalue(trainData$BsmtCond, Qualities))
#Basement Exposure
trainData$BsmtExposure[is.na(trainData$BsmtExposure)] = 'None'
trainData$BsmtExposure = as.factor(trainData$BsmtExposure)
#Basement Finish Type 1
trainData$BsmtFinType1[is.na(trainData$BsmtFinType1)] = 'None'
trainData$BsmtFinType1 = as.factor(trainData$BsmtFinType1)
#Basement Finish Type 2
trainData$BsmtFinType2[is.na(trainData$BsmtFinType2)] = 'None'
trainData$BsmtFinType2 = as.factor(trainData$BsmtFinType2)

#Fixing Masonry Variables
#Masonary Veneer Type
trainData$MasVnrType[is.na(trainData$MasVnrType)] = 'None'
trainData$MasVnrType = as.factor(trainData$MasVnrType)
#Masonary Veneer Area
trainData$MasVnrArea[is.na(trainData$MasVnrArea)] = 0

#Fixing Electrical
trainData$Electrical[is.na(trainData$Electrical)] = "SBrkr"
trainData$Electrical = as.factor(trainData$Electrical)


#Fixing MiscFeature (junk just remove)
trainData = subset(trainData, select = -MiscFeature)
```
```{r, Fixing Character Variables, echo=FALSE,include=FALSE}
#Turning Char variables into factor variables
trainData$MSZoning = as.factor(trainData$MSZoning)
trainData$Street = as.factor(trainData$Street)
trainData$LandContour = as.factor(trainData$LandContour)
trainData$Neighborhood = as.factor(trainData$Neighborhood)
trainData$Condition1 = as.factor(trainData$Condition1)
trainData$BldgType = as.factor(trainData$BldgType)
trainData$HouseStyle = as.factor(trainData$HouseStyle)
trainData$RoofStyle = as.factor(trainData$RoofStyle)
trainData$Exterior1st = as.factor(trainData$Exterior1st)
trainData$Exterior2nd = as.factor(trainData$Exterior2nd)
trainData$Foundation = as.factor(trainData$Foundation)
trainData$Heating = as.factor(trainData$Heating)
trainData$SaleType = as.factor(trainData$SaleType)
trainData$SaleCondition = as.factor(trainData$SaleCondition)


#Turning Char Variables into Ordinal Numeric
trainData$LandSlope = as.integer(revalue(trainData$LandSlope, c('Sev'=0, 'Mod'=1, 'Gtl'=2)))
trainData$ExterQual = as.integer(revalue(trainData$ExterQual, Qualities))
trainData$ExterCond = as.integer(revalue(trainData$ExterCond, Qualities))
trainData$HeatingQC = as.integer(revalue(trainData$HeatingQC, Qualities))
trainData$CentralAir = as.integer(revalue(trainData$CentralAir, c('N'=0, 'Y'=1)))
trainData$KitchenQual = as.integer(revalue(trainData$KitchenQual, Qualities))
trainData$Functional = as.integer(revalue(trainData$Functional, c('Sal'=0, 'Sev'=1, 'Maj2'=2, 'Maj1'=3, 'Mod'=4, 'Min2'=5, 'Min1'=6, 'Typ'=7)))
trainData$PavedDrive = as.integer(revalue(trainData$PavedDrive, c('N'=0, 'P'=1, 'Y'=2)))

#Not a part of the data just observation #
trainData = subset(trainData, select = -Id)

#Make sure Empty
charVariables = names(trainData[,sapply(trainData, is.character)])
charVariables
```

###  Dealing with Multicollinearity
We decided to just deal with multicollinearity issues before building any models. We checked the correlation matrix for all numeric variables and decided to remove any pair of variables with a correlation of 0.75 or higher. Of the two variables we kept Whichever variable had a higher correlation with SalePrice. Below is a list of all of these pairs and which variables we kept.

* **Garage Yr Blt vs. Year Built:** correlation = 0.845. We kept Year Built
* **Garage Area vs. Garage Cars:** correlation = 0.882. We kept Garage Cars
* **Total Basement SqFt vs. 1st Floor SqFt** correlation = 0.819. We kept Total Basement SqFt
* **Garage Condition vs. Garage Quality** correlation = 0.959. We kept Garage Quality
* **Total Rooms Above Ground vs. Ground Living Area** correlation = 0.825. We kept Ground Living Area

```{r Multicollinearity, include=FALSE, echo=FALSE}
#Finding the Correlated Variables
    # numericVars <- which(sapply(trainData, is.numeric))
    # all_numVar <- trainData[, numericVars]
    # #Garage Year Build vs YearBuilt
    # print(cor(all_numVar, trainData$GarageYrBlt) > 0.75)
    # print(cor(trainData$GarageYrBlt, trainData$SalePrice))
    # print(cor(trainData$YearBuilt, trainData$SalePrice))
    # #Garage Garage Cars vs GarageArea
    # print(cor(trainData$GarageArea, trainData$GarageCars))
    # print(cor(trainData$GarageArea, trainData$SalePrice))
    # print(cor(trainData$GarageCars, trainData$SalePrice))
    # #TotalBsmtSF vs. X1stFlrSF
    # print(cor(trainData$TotalBsmtSF, all_numVar) > 0.75)
    # print(cor(trainData$TotalBsmtSF, trainData$X1stFlrSF))
    # print(cor(trainData$TotalBsmtSF, trainData$SalePrice))
    # print(cor(trainData$X1stFlrSF, trainData$SalePrice))
    # #GarageCond vs. Garage Quality
    # print(cor(trainData$GarageCond, all_numVar) > 0.75)
    # print(cor(trainData$GarageCond, trainData$GarageQual))
    # print(cor(trainData$GarageCond, trainData$SalePrice))
    # print(cor(trainData$GarageQual, trainData$SalePrice))
    # #Total Rooms Above Ground vs. Ground Living Area
    # print(cor(trainData$TotRmsAbvGrd, all_numVar) > 0.75)
    # print(cor(trainData$TotRmsAbvGrd, trainData$GrLivArea))
    # print(cor(trainData$TotRmsAbvGrd, trainData$SalePrice))
    # print(cor(trainData$GrLivArea, trainData$SalePrice))
    
#Remove the Variables
trainData = subset(trainData, select = -GarageYrBlt)
trainData = subset(trainData, select = -GarageArea)
trainData = subset(trainData, select = -X1stFlrSF)
trainData = subset(trainData, select = -GarageCond)
trainData = subset(trainData, select = -TotRmsAbvGrd)

#Check Removed
#dim(trainData)
```

###  Dealing with Sparse Data
Some of our factor variables had levels with a count of less then 5. This was causing problems with using cross validation. Below is a list of all the variables that had this issue and how we dealth with them.

* **Utilities:** Type of utilities available. 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Condition2:** Proximity to various conditions. 8 levels with 99% of the data belonging to one level. We removed this variable.
* **RoofMatl1:** Roof Material. 8 levels with more than 98% of the data belonging to one level. We removed this variable.
* **Street:** 2 levels with more than 99% of the data belonging to one level. We removed this variable.
* **Heating:** 6 levels with 98% of the data belonging to one level. We removed this variable.

We kept the rest of the variables that had sparse levels in the model because they had other levels that were not sparse and could be important in future models. We converted all the factor variables into dummy variables and then just removed the dummy variables with levels that had less than 10 observations. This ended up being 32 out of 162 levels.

```{r Looking at Catagorical Variables, echo=FALSE, include=FALSE}
#Find Categorical Variables
numericVars = which(sapply(trainData, is.numeric))
all_factorVars = trainData[,-numericVars]
print(names(all_factorVars))
```
```{r Remove Worthless Variables, include=FALSE,echo=FALSE}
#Too Sparse < 2% of data not in same category
trainData = subset(trainData, select = -Utilities)
trainData = subset(trainData, select = -Condition2)
trainData = subset(trainData, select = -RoofMatl)
trainData = subset(trainData, select = -Street)
trainData = subset(trainData, select = -Heating)
```
```{r Dummy Variables and new Train DF, include=FALSE, echo=FALSE}
#Find all the factor variables
factorVars = which(sapply(trainData, is.factor))
factorVars = names(factorVars)
#Create seperate data frame to convert factor variables to dummy varaibles
DFfactors = trainData[, factorVars]
DFdummies = dummy.data.frame(DFfactors, sep = ".")
#Find Sparse dummy variables
sparseLevels <- which(colSums(DFdummies[1:nrow(trainData[!is.na(trainData$SalePrice),]),])<10)
#Remove Sparse dummy variables
DFdummies = DFdummies[,-sparseLevels]
#combine all the non factor variables in original data frame with our new dummy variables
DFnumeric = trainData[,!(names(trainData) %in% factorVars)]
trainDF.dummies = cbind(DFnumeric, DFdummies)
```



###  Dealing with Skewed Response Variable
The response variable SalePrice was heavily right skewed. We fixed this with a log transformation. Below is a couple before and after graphs showing the results. We will be using the log(SalePrice) as our predictor variable in our models.

```{r Create Log Sale Price Data Frame, include=FALSE, echo=FALSE}
trainDF.log = trainDF.dummies
trainDF.log$SalePrice = log(trainDF.log$SalePrice)
```
```{r Plot of SalePrice vs. Normal Distribution , echo=FALSE , fig.align = "center", fig.height = 5, fig.width = 7}
p1 = ggplot(trainDF.dummies, aes(x=SalePrice)) +
        geom_histogram(aes(y = ..density..), fill="blue", binwidth = 10000) +
        stat_function(fun = dnorm, args = list(mean(trainDF.dummies$SalePrice),sd(trainDF.dummies$SalePrice)), colour = "red", lwd = 1) +
        scale_x_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000")) +
        labs(title = "SalePrice vs. Normal Distribution")

p2 = ggplot(trainDF.log, aes(x=SalePrice)) +
        geom_histogram(aes(y = ..density..), fill="blue", binwidth = .15) +
        stat_function(fun = dnorm, args = list(mean(trainDF.log$SalePrice),sd(trainDF.log$SalePrice)), colour = "red", lwd = 1) +
        scale_x_continuous(breaks= seq(10, 14, by=.5)) +
        labs(title = "Log SalePrice vs. Normal Distribution")

grid.arrange(p1, p2, nrow = 1, ncol = 2)
```
```{r QQ Plots, echo=FALSE, fig.height = 5, fig.width = 7, fig.align = "center"}
#QQ PLots
par(mfrow = c(1,2))
qqnorm(trainDF.dummies$SalePrice, main = "Sale Price QQ Plot")
qqline(trainDF.dummies$SalePrice)
qqnorm(trainDF.log$SalePrice, main = "Log Sale Price QQ Plot")
qqline(trainDF.log$SalePrice)
```

### Dealing with Outliers
The only outliers we found were observations 524 and 1299. We deemed these outliers significant enough to remove because they were huge houses with a overall quality rating of 10 (the highest rating) and yet they sold for a very low price. By looking at the size and quality rating of the house if appears they may have missed a 0 on the end of the price. Size of the house (GrLiveArea) and Overall Quality are the two highest positive correlated variables with SalePrice which lead us to believe these SalePrices were a mistake and we removed these observations from the data set. We left the graphs below in terms of the actual Sale Price and not log of the Sale Price so it would be easier to interpret

```{r Finding Outliers, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 6, comment=""}
ggplot(data=trainDF.dummies[!is.na(trainDF.dummies$SalePrice),], aes(x=GrLivArea, y=SalePrice))+
        geom_point(col='blue') + geom_smooth(method = "lm", se=FALSE, color="red", aes(group=1)) +
        scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000") ) +
        geom_text_repel(aes(label = ifelse(trainDF.dummies$GrLivArea[!is.na(trainDF.dummies$SalePrice)]>4500, rownames(trainDF.dummies), '')))
trainDF.dummies[c(524, 1299), c('SalePrice', 'GrLivArea', 'OverallQual')]
```

```{r final Data to be used for models, echo=FALSE, include=FALSE}
#Use cleanData for all future models
cleanData = trainDF.log
cleanData = cleanData[-c(524,1299),]
```


#Multiple Linear Regression
The main purpose of building a Multiple Linear Regression model is to use it as a benchmark that we compare all future models to. Below is a brief summary of our Multiple Linear Regression model that includes all variables in the model fit. 

* 10 Fold Cross Valiation Error Rate: (waiting for clean data)
* Most Significant Parameters: 
* Non-Significant Parameters: 

```{r, Base Model, echo=FALSE, include=FALSE}
baseModel = glm(SalePrice ~ ., data = trainDF.dummies)
summary(baseModel)
```

```{r, Cross Validation, comment="", include=FALSE, echo=FALSE}
set.seed(1)
cvTrainData = trainDF.dummies
cv.error10 = cv.glm(cvTrainData, baseModel, K=10)$delta[1]
```

##  Potential Problems

####     Residual Plots
From the Residual plots it appears as if a linear model may not be a good representation of the data. We plan to try out some Model selection and regularization methods and see if this pattern still exists. If so we will use non-linear methods.

```{r, Model Residuals, echo=FALSE, include=FALSE}
par(mfrow= c(2,2))
plot(baseModel)
```

####     Model Outliers
After looking at the residual plot we noticed there are 10 outliers that may be influencing the model fit. The points influncing the data are listed below. We will keep an eye on these observations in future models and see if they continue to be a problem. We will deal with them later if we need to.
```{r, Model Outliers, echo=FALSE, comment=""}
outlierTest(baseModel)
```



#Model Selection and Regularization

##Subset Selection
```{r Stuff We Need for Subset Selection, include=FALSE, echo=FALSE}
subSetData = cleanData

predict.regsubsets = function(object, newdata, id, ...){
    form=as.formula (object$call [[2]])
    mat=model.matrix(form ,newdata )
    coefi=coef(object ,id=id)
    xvars=names(coefi)
    mat[,xvars]%*%coefi
}
```

####Forward Stepwise Selection
Forward stepwise Selection revealed that the best model using this method is somewhere around 40-50 variables in the model. Remember this is not necessarly the best model because forward stepwise can get stuck in a suboptimal path. That being said it appears like around 50 variables will turn out to be a good model.

```{r, Forward Stepwise Selection, include=FALSE, echo=FALSE}
#CP, BIC, AdjR^2
regfit.fwd = regsubsets(SalePrice ~., data = subSetData, nvmax = 160, method = "forward")
regfit.fwd.summary = summary(regfit.fwd)

#validation set 
set.seed(88)
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train <- subSetData[sample, ]
test  <- subSetData[-sample, ]

regfit.valid = regsubsets(SalePrice ~., data = train, nvmax = 160, method = "forward")
test.matrix = model.matrix(SalePrice ~., data = test)
validation.errors = rep(NA, 160)
for (i in 1:160){
    coefi = coef(regfit.valid, id=i)
    pred = test.matrix[,names(coefi)]%*%coefi
    validation.errors[i] = mean((test$SalePrice-pred)^2)
}
```

```{r plots to include, echo=FALSE}
#set xlab abd ylab and points
par(mfrow = c(2,2))
plot(regfit.fwd.summary$adjr2, type = "l", xlab = "Number of Variables", ylab = "Adjusted R Squared")
points(which.max(regfit.fwd.summary$adjr2), regfit.fwd.summary$adjr2[which.max(regfit.fwd.summary$adjr2)], col="red", cex=2,pch=20)
plot(regfit.fwd.summary$bic, type = "l", ylab = "BIC", xlab = "Number of Variables")
points(which.min(regfit.fwd.summary$bic), regfit.fwd.summary$bic[which.min(regfit.fwd.summary$bic)], col="red", cex=2,pch=20)
plot(regfit.fwd.summary$cp, type = "l", ylab = "CP", xlab = "Number of Variables")
points(which.min(regfit.fwd.summary$cp), regfit.fwd.summary$cp[which.min(regfit.fwd.summary$cp)], col="red", cex=2,pch=20)
plot(validation.errors, type = "l", ylab = "Validation MSE", xlab = "Number of Variables")
points(which.min(validation.errors), validation.errors[which.min(validation.errors)], col="red",cex=2,pch=20)
```

The most interesting thing I found from doing forward stepwise was just determining which variables were selected first and therefore the potentially important variables in predicting sale price.

**First 10 Variables picked in model**

* **OverallQual:** Rates the overall material and finish of the house.
* **BsmtFullBath:** Number of full bathrooms in the basement.
* **YearBuilt:** Original Construction Date.
* **OverallCond:** Rates the overall condition of the house.
* **GarageQual:** Rates the Quality of the Garage.
* **HeatingQC:** Heating Quality and Condition Rating.
* **Alley.none:** Dummy Variable representing no alleyway access to the house.
* **BsmtHalfBath:** Number of half bathrooms in the basement.
* **GarageFinish:** Rating of Interior finish of the garage.
* **MSZoning.FV:** Dummy Variable for Zoning Code for "Floating Village Residential"

We found these results to be a little odd (are basement half bathrooms and alleyway access really more imporant than Above Ground Sq. feet?). We think this is because Overall Quality is correlated with some of the other more traditionally important variables like Above Ground SqFt. By itself Above Ground SqFt. has a correlation with sale price of 0.7 which is a lot higher then BsmtFullBath at 0.24, but Above Ground Sqft. is and Overall Quality have a correlation of 0.59. Because of this a lot of the information containted in Above Ground Sqft. is already in the model after adding overall quality. Therefore the second variable added is actually the second most important variable given the information from overall quality is already in the model, which is not necessarily the second most important variable in a houses Sale Price.


####Backwards Stepwise Selection
Backwards Stepwise Selection ends up with pretty much the same results as forward stepwise selection. Somewhere between 40-50 variables seems to be when the graphs begin to flatten out. I would lean towards a simpler model and choose the best 40 variable model. This is to many variables to report the model so we just look at some of the most significant ones below.

```{r, Backwards Stepwise Selection, include=FALSE, echo=FALSE}
#CP, BIC, AdjR^2
regfit.back = regsubsets(SalePrice ~., data = subSetData, nvmax = 160, method = "backward")
regfit.back.summary = summary(regfit.back)

#validation set 
set.seed(88)
sample <- sample.int(n = nrow(subSetData), size = floor(.75*nrow(subSetData)), replace = F)
train <- subSetData[sample, ]
test  <- subSetData[-sample, ]

regfit.valid = regsubsets(SalePrice ~., data = train, nvmax = 160, method = "backward")
test.matrix = model.matrix(SalePrice ~., data = test)
validation.errors = rep(NA, 160)
for (i in 1:160){
    coefi = coef(regfit.valid, id=i)
    pred = test.matrix[,names(coefi)]%*%coefi
    validation.errors[i] = mean((test$SalePrice-pred)^2)
}
```
```{r plots for backstep, echo=FALSE}
par(mfrow = c(2,2))
plot(regfit.back.summary$adjr2, type = "l", xlab = "Number of Variables", ylab = "Adjusted R Squared")
points(which.max(regfit.back.summary$adjr2), regfit.back.summary$adjr2[which.max(regfit.back.summary$adjr2)], col="red", cex=2,pch=20)
plot(regfit.back.summary$bic, type = "l", ylab = "BIC", xlab = "Number of Variables")
points(which.min(regfit.back.summary$bic), regfit.back.summary$bic[which.min(regfit.back.summary$bic)], col="red", cex=2,pch=20)
plot(regfit.back.summary$cp, type = "l", ylab = "CP", xlab = "Number of Variables")
points(which.min(regfit.back.summary$cp), regfit.back.summary$cp[which.min(regfit.back.summary$cp)], col="red", cex=2,pch=20)
plot(validation.errors, type = "l", ylab = "Validation MSE", xlab = "Number of Variables")
points(which.min(validation.errors), validation.errors[which.min(validation.errors)], col="red",cex=2,pch=20)
```

**Last 10 Variables left in model**

* **OverallQual:** Rates the overall material and finish of the house.
* **BsmtFullBath:** Number of full bathrooms in the basement.
* **YearBuilt:** Original Construction Date.
* **BsmtFinSF1:** How many square feet of the basement that is finished.
* **OverallCond:** Rates the overall condition of the house.
* **GarageQual:** Rates the Quality of the Garage.
* **BsmtUnfSF:** Unfinished square feet of basement area.
* **LotArea:** Lot Size in Square Feet
* **BsmtFinSF2:** Rating of basment finished area.
* **Neighborhood.MeadowV:** Neighboorhood location Meadow Village

The differences between forward stepwise and backwards stepwise are that BsmtFinSF1, BsmtUnfSF, LotArea, BsmtFinSF2, Neighborhood.MeadowV are now in the top 10 and HeatingQC, Alley.none, BsmtHalfBath, GarageFinish, MSZoning.FV, have been removed from the top 10. I think it is interesting to note that both methods find that having a finished basement with bathrooms seems to be important factor in predicting SalePrice (after accounting for Overall Quality). 

Also we see that a neighborhood appeared in the top 10 here. The coefficient for this neighborhood is negative so we looked a little deeper to see this neighboorhoods relationship with sale price in comparison to other neighborhoods. As you can see from the plot below Meadown Village has the lowest median sale price of all neighborhoods therefore this coefficient makes sense. Also it makes sense that Meadow Village would be a significant factor in predicting sale price because it appears to be one of the worst neighborhoods in terms of sales price in Ames Iowa.


```{r a look at Neighborhoods, echo=FALSE}
neighbor.plot = ggplot(trainData, aes(Neighborhood, SalePrice)) +
                geom_boxplot(aes(fill = factor(Neighborhood)))  +
                theme(axis.text.x = element_text(angle = 65, vjust = 0.6)) +
                labs( x = "Neighborhoods", y = "Sale Price") +
                theme(legend.position="none") +
                scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = c("0", "200,000", "400,000","600,000","800,000") ) +
                geom_hline(yintercept = 88000, linetype="dashed", color = "red")
neighbor.plot
```

##Shrinkage Methods
####Ridge Regression
The best lambda we found is pretty close to 0 and therefore it looks like the least squares model is pretty close to the best model for this data set. This is not a huge surprise because we noticed in our subset selection models that a lot of predictors were significant. Also our sample size is a lot larger then the number of predictors 1458 >> 177 so reducing variance may not be a huge concern here. The Ridge Regression model and the least squares model perform pretty much identically on a validation set at lamda = 0.044. They both significantly beat the Null Model that only uses the mean of Sale Price to predict Sale Price. Inference on Ridge regression is not ideal but below is a plot of the Ridge Regression coefficients with the largest coefficients labeled.

* **Best Lambda:** 0.044
* **Test RMSE Ridge:** $21,646.23
* **Test RMSE Least Squares:** $21,543.60
* **Test RMSE Null Model:** $79,513.67

*We changed the Sale Price back into dollars instead of log(SalePrice) because it was easier to interpret. Also note this is the Root Mean Squared Error which can be thought of as how close our predictions are on average.

```{r, Ridge Regression, include=FALSE, echo=FALSE}
#Setup
set.seed(1)
x = model.matrix(SalePrice ~., cleanData)[,-1]
y = cleanData$SalePrice
train = sample(1:nrow(x), nrow(x)/1.5)
test = (-train)
y.test= y[test]
ridge.mod = glmnet(x[train,],y[train],alpha = 0)

#Find best lamda value
cv.out = cv.glmnet(x[train,],y[train], alpha = 0)
bestLambda = cv.out$lambda.min
bestLambda

#test RMSE for best lambda
ridge.pred = predict(ridge.mod, s = bestLambda, newx = x[test,])
(mean((exp(ridge.pred)-exp(y.test))^2))^(1/2)
#test RMSE for least squares model
ls.pred = predict(ridge.mod, s = 0.00001, newx = x[test,])
(mean((exp(ls.pred)-exp(y.test))^2))^(1/2)
#baseline null model predictions
mean(( mean(exp(y[train])) -exp(y.test) )^2)^(1/2)
```
```{r, Ridge Regression plot, echo=FALSE}
plot(ridge.mod, xvar="lambda")
legend(1.75, -.125, legend=c("Best Lambda", "MSZoning.C (all)", "Neighborhood.MeadowV", "Exterior1st.BrkFace", "Neighborhood.Crawfor", "Neighborhood.StoneBr"),
       col=c("red","magenta", "black", "red", "green", 69), lty=c(2,1,1,1,1,1), cex=1)
abline(v = log(bestLambda), col = "red", lwd = 2, lty = 2)
```

####Lasso Regression
The Lasso model with the lowest cross validation error rate is a model with 69 predictors. This matches with what we saw in forward and backward stepwise selection. Although this helps with inference because of having fewer variables it doesn't help all that much with prediction. Again the Test RMSE is approximately the same between the Lasso model and the full Least Squares model. Below is a plot of the Lasso Regression coefficients with the largest coefficients labeled. Note we are seeing a lot of the same variables appearing again and again. It appears Location really is that important.

* **Best Lambda:** 0.0042
* **Test RMSE Lasso:** $20,296.13
* **Test RMSE Least Squares:** $21,543.60
* **Test RMSE Null Model:** $79,513.67
* **Count of Non-Zero Coefficients:** 69


```{r, Lasso, include=FALSE, echo=FALSE}
#Setup
set.seed(1)
x = model.matrix(SalePrice ~., cleanData)[,-1]
y = cleanData$SalePrice
train = sample(1:nrow(x), nrow(x)/1.5)
test = (-train)
y.test= y[test]

#Baseline Lasso Model
lasso.mod = glmnet(x[train,], y[train], alpha = 1)

#Find best Lambda
cv.lasso.out = cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.lasso.out)
bestLassoLambda = cv.lasso.out$lambda.min
bestLassoLambda

#Lasso Predictions accuracy
lasso.pred = predict(lasso.mod, s = bestLassoLambda, newx = x[test,])
(mean((exp(lasso.pred)-exp(y.test))^2))^(1/2)

#Number of Non Zero coefficients at best lambda
lasso.coef = predict(lasso.mod, type = "coefficients", s=bestLassoLambda)
sum(lasso.coef!=0)
```

```{r, Lasso Regression plot, echo=FALSE}
plot(lasso.mod, xvar="lambda")
legend(-4, -.275, legend=c("Best Lambda", "MSZoning.C (all)", "Neighborhood.MeadowV", "Neighborhood.Crawfor"), col=c("red", 69, "black", "blue" ), lty=c(2,1,1,1), pt.cex=1,  cex = 0.75)
abline(v = log(bestLassoLambda), col = "red", lwd = 2, lty = 2)
```


##Dimension Reduction Methods
####Principle Components Regression
```{r, PCR, include=FALSE, echo=FALSE}

```

####Partial Least Squares
```{r, PLS, include=FALSE, echo=FALSE}

```